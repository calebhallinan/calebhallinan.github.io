[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Here is my updated CV as of 10/11/2023"
  },
  {
    "objectID": "blog/might-try-blogging/index.html",
    "href": "blog/might-try-blogging/index.html",
    "title": "Might try blogging…",
    "section": "",
    "text": "I am thinking about blogging some things… not really specific topics or thoughts, just notes I want to jot down because why not."
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "",
    "text": "Code\n### install and read in packages\n\n# install.packages(\"wordcloud\") # install this for wordcloud\n# install.packages(\"janitor\") # install for sentiment analysis\n# install.packages(\"devtools\")\n# devtools::install_github(\"ropensci/gutenbergr\")\n# install.packages(\"tm\")\n\nlibrary(tidyverse) # for dataframes\nlibrary(devtools) # for classic functions\nlibrary(gutenbergr) # for data\nlibrary(here) # for use of others if downloaded\nlibrary(tidytext) # for sentiment analysis\nlibrary(paletteer) # for colors for plots\nlibrary(tm) # topic model package\nlibrary(topicmodels) # topic model package"
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html#choosing-an-author",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html#choosing-an-author",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "Choosing an Author",
    "text": "Choosing an Author\nInitially, the researcher expressed interest in exploring Charles Darwin’s works due to their background in Biology. However, after conducting a sentiment analysis on his works it became evident that they resembled scientific papers more so than books. As a result, the analysis on Darwin yielded less informative and exciting results than anticipated. Consequently, the focus shifted to the works of Alexandre Dumas, a celebrated fiction and fantasy novelist renowned for works such as The Three Musketeers and The Count of Monte Cristo - a classic the researcher has personally read and thoroughly enjoyed. From the extensive collection the gutenbergr package has to offer of Dumas’ books, six books were selected: The Three Musketeers, Ten Years Later, Twenty Years After, The Black Tulip, The Count of Monte Cristo, Illustrated, and the The Wolf-Leader. According to Alexandre Dumas’ Wikipedia page, his books are classified into various types of fiction. The first three books belong to The D’Artagnan Romances trilogy, while the The Count of Monte Cristo, Illustrated and The Black Tulip fall into the adventure genre. The Wolf-Leader was the final novel analyzed and was categorized as one of Dumas’ fantasy books. With this background knowledge, by performing a sentiment analysis we can not only uncover differences and similarities between individual books but also between different genres within Dumas’ literary repertoire.\n\n\nCode\n### Getting Dumas, Alexandre Data ###\n\n\n# if file doesn't exist, download the data\nif (!file.exists(here(\"dumas.RDS\"))) {\n  \n  # message it wasn't found\n  message(\"File not found, downloading now...\")\n  \n  dumas = gutenberg_works() |&gt;\n  # group by author\n  group_by(author) |&gt;\n  # filter to get dumas\n  filter(author == \"Dumas, Alexandre\") |&gt;\n  # download data\n  gutenberg_download(meta_fields = \"title\", strip=TRUE)\n  \n  # save the files to RDS objects\n  saveRDS(dumas, file = here(\"dumas.RDS\"))\n  \n  # message when done\n  message(\"Finished!\") \n}\n\n\n# read in dumas\ndumas = readRDS(here(\"dumas.RDS\"))\n# use git_ignore to not push\n# usethis::use_git_ignore(\"dumas.RDS\")\n\n\n# get row numbers for dumas\ndumas = dumas |&gt; \n  # get rid of id\n  select(-gutenberg_id) |&gt;\n  # get rid of lines with no text \n  filter(text != \"\") |&gt; \n  # group by title\n  group_by(title) |&gt;\n  # make new column\n  mutate(linenumber = row_number()) |&gt; \n  ungroup()"
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html#sentiment-analysis",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html#sentiment-analysis",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nAfter conducting sentiment analysis on five of Alexandre Dumas’ well-known books, some interesting trends regarding his overall writing style and book-specific style emerged. Dumas’ writing exhibited a predominantly negative tone throughout, as evident by the cumulative sentiment declining constantly across all books. While this is only a small sample of his literary works, this trend is believed to be true based on our existing knowledge about Dumas.\n\n\nCode\n### Sentiment Analysis of Dumas, Alexandre Data ###\n\n# sample data here to save computational effort for rest of analysis\ndumas = dumas |&gt; \n  filter(title %in% c(\"The Three Musketeers\", \"Ten Years Later\", \"Twenty Years After\", \n                      \"The Black Tulip\", \"The Count of Monte Cristo, Illustrated\", \n                      \"The Wolf-Leader\"))\n\n\n# tokenize author\ntidy_dumas = dumas |&gt;\n  unnest_tokens(word, text)\n\n\n# # check to see what the top words are\n# tidy_dumas |&gt;\n#   # group by word\n#   group_by(word) |&gt;\n#   # count\n#   tally() |&gt; \n#   # arrange them\n#   arrange(desc(n)) \n# # NOTE: lots of the's and a's and such\n\n\n# # make a word cloud for just count of monte cristo\n# tt_dumas = tidy_dumas |&gt;\n#   # get count of monte\n#   filter(title == \"The Count of Monte Cristo, Illustrated\") |&gt;\n#   # get actual count of works\n#   count(word) |&gt;\n#   # arrange\n#   arrange(desc(n)) |&gt;\n#   # only get 200\n#   slice(1:200L)\n# # make a wordcloud of it\n# wordcloud::wordcloud(tt_dumas$word, tt_dumas$n)\n# # NOTE: lots of the's and a's and such\n\n\n# # can see words by by books\n# tidy_dumas |&gt;\n#   # count\n#   count(title, word) |&gt; \n#   #arrange\n#   arrange(desc(n)) |&gt; \n#   # group by title now\n#   group_by(title) |&gt; \n#   # get a couple\n#   slice(1L) \n\n\n# filter author with stop words\ntidy_dumas = tidy_dumas |&gt; \n  # get rid of stop words\n  anti_join(stop_words, by = \"word\")\n\n\n# # check with filtered author now\n# tidy_dumas |&gt;\n#   count(word) |&gt;\n#   arrange(desc(n))\n# # NOTE: lots of de, madame, replied, etc. that I should prob get rid of\n\n\n# top words by book\ntop_dumas_words = tidy_dumas |&gt;\n  # count with word and group by title\n  count(word, title) |&gt;\n  # arrange\n  arrange(desc(n)) |&gt; \n  # group by title\n  group_by(title) \n# top_dumas_words |&gt; slice(1:2)\n# NOTE: lots of names I should get rid of\n\n\n# # word cloud with no stop words\n# tt_dumas = tidy_dumas |&gt;\n#   # get count of monte again\n#   filter(title == \"The Count of Monte Cristo, Illustrated\") |&gt; \n#   # count\n#   count(word) |&gt;\n#   # arrange\n#   arrange(desc(n)) |&gt; \n#   # get 200\n#   slice(1:200L) \n# # make wordcloud\n# wordcloud::wordcloud(tt_dumas$word, tt_dumas$n)\n# # NOTE: \"count\" is highest word unsurpisingly\n\n\n# get bing sentiments\nbing = tidytext::sentiments \n# getting dupe words from janitor package\ndupes = bing |&gt; \n  janitor::get_dupes(word) \n# get rid of dupes\nbing = bing |&gt; \n  anti_join(dupes |&gt; filter(sentiment == \"positive\"))\n# check\n# anyDuplicated(bing$word) == 0\n# NOTE: good here!\n\n\n# top word sentiments with all words\n# top_dumas_words |&gt;\n#   slice(1:2) |&gt;\n#   left_join(bing, by = join_by(word))\n\n\n# # top word sentiments with only words with sentiment\n# top_dumas_words |&gt;\n#   # get rid of drop words\n#   filter(!word %in% dropwords) |&gt; \n#   inner_join(bing, by = join_by(word)) |&gt;\n#   slice(20:30) \n#   # NOTE: checked slices 1:30\n\n\n# Using this method to look at the text to determine if I should remove the word or not\n# or us a regex method\n# dumas |&gt;\n#   filter(str_detect(text, \"ah\"))\n\n# NOTES:\n# majesty: is usually \"his majesty\" or \"your majesty\" so remove all\n# honor: thought it would be like \"your honor\" but not really so keep it\n# prisoner: almost always its \"the prisoner\" so remove\n# master: mostly his master and master (referring to person), so add\n# excellency: same as your honor vibe, remove\n# stranger: not really used as person as much as I would have thought, so keep\n# de: found this when doing comparison to other others, just a name between names\n\n\n# going to get rid of some words here\ndropwords = c(\"majesty\", \"prisoner\", \"master\", \"excellency\", \"de\")\n\n\n# author sentiment\ndumassentiment = tidy_dumas |&gt; \n  # get rid of drop words\n  filter(!word %in% dropwords) |&gt; \n  # join with bing to get sentiment\n  inner_join(bing, by = join_by(word)) |&gt; \n  # TODO: what is this 80 for?\n  count(title, page = linenumber %/% 80, sentiment) |&gt; \n  # pivot wider data here\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n  # get sentiment score\n  mutate(sentiment = positive - negative) \n# check head of data\n# head(dumassentiment)\n# NOTE: looks good!\n\n\n# begin graphing\n\n\n# define the desired order of the legend\ndesired_order &lt;- c(\"The Three Musketeers\", \"Ten Years Later\", \"Twenty Years After\", \"The Black Tulip\", \n                   \"The Count of Monte Cristo, Illustrated\", \"The Wolf-Leader\")\n# define the desired colors for each title\ndesired_colors &lt;- c(\"darkblue\", \"blue\", \"lightblue\", \"darkred\", \"red\",\"darkgreen\")\n\n\n# change order for graph\ndumassentiment$title = factor(dumassentiment$title, levels = desired_order)\n\n\n\n\n# # ggplot of basic analysis of sentiment overtime\n# ggplot(dumassentiment, aes(page, sentiment, fill = title)) + \n#   # make bar graph, remove legend cause dont need\n#   geom_bar(stat = \"identity\", show.legend = FALSE) + \n#   # plot by book/title\n#   facet_wrap(~title, ncol = 3, scales = \"free_x\") +\n#   labs(\n#     x = \"Page Number\",\n#     y = \"Sentiment Score\",\n#     title = \"Sentiment Score Throughout Each Book\",\n#     caption = \"Data Source: Project Gutenberg\"\n#   ) + \n#   theme_bw() +\n#   theme(\n#     text = element_text(size = 11.5),\n#     plot.title = element_text(face = \"bold\", hjust = 0.5),\n#     legend.text = element_text(face = \"italic\")\n#     ) + \n#     scale_fill_manual(\n#     values = setNames(desired_colors, desired_order),\n#     breaks = desired_order\n#   )\n\n\n# plot the data\ng = dumassentiment|&gt; \n  # group by title\n  group_by(title) |&gt; \n  # get cumulative sentiment over time\n  mutate(sentiment = cumsum(sentiment), page = page/max(page)) |&gt; \n  # plot sentiment over time\n  ggplot(aes(page, sentiment, colour = title)) + \n  # make the line width bigger\n  geom_line(linewidth = 1.25) + \n  # labels\n  labs(\n    x = \"Percent of Total Pages (%)\",\n    y = \"Cumulative Sentiment\",\n    title = \"Trajectory of Sentiment Throughout Dumas' Works\",\n    caption = \"Data Source: Project Gutenberg\"\n  )\n\n# making transparent legend\ntransparent_legend = theme(legend.background = element_rect(fill = \"transparent\"), legend.key = \n                             element_rect(fill = \"transparent\", color = \"transparent\"))\n\n# plot\ng + \n  # add transparent legend\n  transparent_legend + \n  # change colors\n  # scale_color_brewer(type = \"qual\") + \n  # scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12), breaks = desired_order) +\n  # make specific colors go to specific titles\n  scale_colour_manual(\n    values = setNames(desired_colors, desired_order),\n    breaks = desired_order\n  ) +\n  # change x axis to percent\n  scale_x_continuous(labels = scales::percent_format()) + \n  # change theme to classic\n  theme_classic() + \n  # edit text\n  theme(\n    legend.position = c(0.25, 0.3), \n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    legend.text = element_text(face = \"italic\")\n    ) + \n  # change legend postion\n  guides(colour = guide_legend(title = \"Book\", override.aes = list(linewidth = 2)))\n\n\n\n\n\nWithin Dumas’ D’Artagnan Romances trilogy, represented by three shades of blue, The Three Musketeers had the lowest cumulative sentiment by the end of the book. The second book, Ten Years Later, contained a relatively more positive sentiment but remained negative overall. The final installment, Twenty Years After, displayed a negative cumulative sentiment similar to the first book. Hence, it appears that Dumas took readers on an emotional roller coaster from book to book while maintaining a generally negative connotation.\nAs for Dumas’ adventure genre books, The Count of Monte Cristo, Illustrated and The Black Tulip which are represented by shades of red, demonstrated significantly different sentiments over time. The Count of Monte Cristo, Illustrated had the lowest cumulative sentiment among all the books, while The Black Tulip approached a neutral sentiment by the end. Notably, The Count of Monte Cristo, Illustrated experienced a substantial drop in negative sentiment halfway through the book and continued its steep downward trend. Although the ending was not particularly positive, the researcher found the late and steep decline in negativity surprising given the “positive” elements (no spoilers) that are conveyed during that period if the book.\nLastly, a cumulative sentiment was performed on The Wolf-Leader, one of Dumas’ fantasy books. Despite its themes of werewolves, greed, power, and lust, the book had a slightly negative cumulative sentiment with minimal variation over time. Overall, it was somewhat surprising that the overall negative sentiment in this novel was not more pronounced.\nIt is important to note that certain words such as “majesty,” “prisoner,” “master,” and “excellency” were omitted from the analysis. These words were typically used as titles or names (e.g., “the prisoner” or “your excellency”) and did not significantly contribute to the books’ content. However, the exclusion of these words and the comparison between analyses with and without them did not lead to a significant deviation in the overall sentiment trend presented."
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html#topic-model-analysis",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html#topic-model-analysis",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "Topic Model Analysis",
    "text": "Topic Model Analysis\nNext, a topic model analysis using Latent Dirichlet allocation (LDA) was conducted. The Dumas’ dataset was combined with two new authors, Aristotle and Scott F. Fitzgerald, each contributing five distinct works. Aristotle’s selected books were The Poetics of Aristotle, The Categories, Politics: A Treatise on Government, Aristotle on the art of poetry, The Athenian Constitution. Scott F. Fitzgerald’s chosen works included This Side of Paradise, Flappers and Philosophers, The Beautiful and Damned, The Great Gatsby, All the Sad Young Men.\n\n\n\nCode\n### Getting Aristotle and Fitgerald Data ###\n\n\n# if file doesn't exist, download the data\nif (!file.exists(here(\"aristotle.RDS\"))) {\n  \n  # message it wasn't found\n  message(\"File not found, downloading now...\")\n  \n  aristotle = gutenberg_works() |&gt;\n  # group by author\n  group_by(author) |&gt;\n  # filter to get aristotle\n  filter(author == \"Aristotle\") |&gt;\n  # download data\n  gutenberg_download(meta_fields = \"title\", strip=TRUE)\n  \n  # save the files to RDS objects\n  saveRDS(aristotle, file = here(\"aristotle.RDS\"))\n  \n  # message when done\n  message(\"Finished!\") \n}\n\n\n# read in aristotle\naristotle = readRDS(here(\"aristotle.RDS\"))\n# use git_ignore to not push\n# usethis::use_git_ignore(\"aristotle.RDS\")\n\n\n# get row numbers for dumas\naristotle = aristotle |&gt; \n  # get rid of id\n  select(-gutenberg_id) |&gt;\n  # get rid of lines with no text \n  filter(text != \"\") |&gt; \n  # group by title\n  group_by(title) |&gt;\n  # make new column\n  mutate(linenumber = row_number()) |&gt; \n  ungroup()\n\n\n################################################\n\n\n# if file doesn't exist, download the data\nif (!file.exists(here(\"fitzgerald.RDS\"))) {\n  \n  # message it wasn't found\n  message(\"File not found, downloading now...\")\n  \n  fitzgerald = gutenberg_works() |&gt;\n  # group by author\n  group_by(author) |&gt;\n  # filter to get author\n  filter(author == \"Fitzgerald, F. Scott (Francis Scott)\") |&gt;\n  # download data\n  gutenberg_download(meta_fields = \"title\", strip=TRUE)\n  \n  # save the files to RDS objects\n  saveRDS(fitzgerald, file = here(\"fitzgerald.RDS\"))\n  \n  # message when done\n  message(\"Finished!\") \n}\n\n\n# read in aristotle\nfitzgerald = readRDS(here(\"fitzgerald.RDS\"))\n# use git_ignore to not push\nusethis::use_git_ignore(\"fitzgerald.RDS\")\n\n\n# get row numbers for dumas\nfitzgerald = fitzgerald |&gt; \n  # get rid of id\n  select(-gutenberg_id) |&gt;\n  # get rid of lines with no text \n  filter(text != \"\") |&gt; \n  # group by title\n  group_by(title) |&gt;\n  # make new column\n  mutate(linenumber = row_number()) |&gt; \n  ungroup()\n\n\n### Clean aristotle and fitzgerald Data, also do sentiment analysis but don't print ###\n\n\n# tokenize author\ntidy_aristotle = aristotle |&gt;\n  unnest_tokens(word, text)\n\n\n# filter author with stop words\ntidy_aristotle = tidy_aristotle |&gt; \n  # get rid of stop words\n  anti_join(stop_words, by = \"word\")\n\n\n# top words by book\ntop_aristotle_words = tidy_aristotle |&gt;\n  # count with word and group by title\n  count(word, title) |&gt;\n  # arrange\n  arrange(desc(n)) |&gt; \n  # group by title\n  group_by(title) \n# top_dumas_words |&gt; slice(1:2)\n\n\n# get bing sentiments\nbing = tidytext::sentiments \n# getting dupe words from janitor package\ndupes = bing |&gt; \n  janitor::get_dupes(word) \n# get rid of dupes\nbing = bing |&gt; \n  anti_join(dupes |&gt; filter(sentiment == \"positive\"))\n# check\n# anyDuplicated(bing$word) == 0\n# NOTE: good here!\n\n\n# # # top word sentiments with only words with sentiment\n# top_aristotle_words |&gt;\n#   # get rid of drop words\n#   filter(!word %in% dropwords) |&gt;\n#   inner_join(bing, by = join_by(word)) |&gt;\n#   slice(1:30)\n#   # NOTE: checked slices 1:30\n\n\n# Using this method to look at the text to determine if I should remove the word or not\n# or us a regex method\n# aristotle |&gt;\n#   filter(str_detect(text, \"cried\"))\n\n# NOTES:\n# majesty: is usually \"his majesty\" or \"your majesty\" so remove all\n# honor: thought it would be like \"your honor\" but not really so keep it\n# prisoner: almost always its \"the prisoner\" so remove\n# master: mostly his master and master (referring to person), so add\n# excellency: same as your honor vibe, remove\n# stranger: not really used as person as much as I would have thought, so keep\n\n\n# going to get rid of some words here\n# dropwords = c(\"majesty\", \"prisoner\", \"master\", \"excellency\")\n\n\n# # author sentiment\n# aristotlesentiment = tidy_aristotle |&gt; \n#   # get rid of drop words\n#   # filter(!word %in% dropwords) |&gt; \n#   # join with bing to get sentiment\n#   inner_join(bing, by = join_by(word)) |&gt; \n#   # TODO: what is this 80 for?\n#   count(title, page = linenumber %/% 80, sentiment) |&gt; \n#   # pivot wider data here\n#   pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n#   # get sentiment score\n#   mutate(sentiment = positive - negative) \n# # check head of data\n# head(aristotlesentiment)\n# # NOTE: looks good!\n\n\n# # ggplot of basic analysis of sentiment overtime\n# ggplot(aristotlesentiment, aes(page, sentiment, fill = title)) + \n#   # make bar graph, remove legend cause dont need\n#   geom_bar(stat = \"identity\", show.legend = FALSE) + \n#   # plot by book/title\n#   facet_wrap(~title, ncol = 3, scales = \"free_x\") \n# \n# \n# # plot the data\n# g = aristotlesentiment|&gt; \n#   # group by title\n#   group_by(title) |&gt; \n#   # get cumulative sentiment over time\n#   mutate(sentiment = cumsum(sentiment), page = page/max(page)) |&gt; \n#   # plot sentiment over time\n#   ggplot(aes(page, sentiment, colour = title)) + \n#   # make the line width bigger\n#   geom_line(linewidth = 1.25) + \n#   # labels\n#   labs(\n#     x = \"Percent of Total Pages (%)\",\n#     y = \"Cumulative Sentiment\",\n#     title = \"Trajectory of Sentiment Throughout Aristotle's books\",\n#     caption = \"Data Source: Project Gutenberg\"\n#   )\n# \n# # making transparent legend\n# transparent_legend = theme(legend.background = element_rect(fill = \"transparent\"), legend.key = \n#                              element_rect(fill = \"transparent\", color = \"transparent\"))\n# \n# # plot\n# g + \n#   # add transparent legend\n#   transparent_legend + \n#   # change colors\n#   scale_color_brewer(type = \"qual\") +\n#   # scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12), breaks = desired_order) +\n#   # make specific colors go to specific titles\n#   # scale_colour_manual(\n#   #   values = setNames(desired_colors, desired_order),\n#   #   breaks = desired_order\n#   # ) +\n#   # change x axis to percent\n#   scale_x_continuous(labels = scales::percent_format()) + \n#   # change theme to classic\n#   theme_classic() + \n#   # edit text\n#   theme(\n#     legend.position = c(0.2, 0.75), \n#     text = element_text(size = 12),\n#     plot.title = element_text(face = \"bold\", hjust = 0.5),\n#     legend.text = element_text(face = \"italic\")\n#     ) + \n#   # change legend postion\n#   guides(colour = guide_legend(title = \"Book\", override.aes = list(linewidth = 2)))\n\n\n\n\n### Clean fitzgerald Data, also do sentiment analysis but don't print ###\n\n\n\n# tokenize author\ntidy_fitzgerald = fitzgerald |&gt;\n  unnest_tokens(word, text)\n\n\n# filter author with stop words\ntidy_fitzgerald = tidy_fitzgerald |&gt; \n  # get rid of stop words\n  anti_join(stop_words, by = \"word\")\n\n\n# top words by book\ntop_fitzgerald_words = tidy_fitzgerald |&gt;\n  # count with word and group by title\n  count(word, title) |&gt;\n  # arrange\n  arrange(desc(n)) |&gt; \n  # group by title\n  group_by(title) \n# top_fitzgerald_words |&gt; slice(1:2)\n\n\n# # # top word sentiments with only words with sentiment\n# top_fitzgerald_words |&gt;\n#   # get rid of drop words\n#   filter(!word %in% dropwords) |&gt;\n#   inner_join(bing, by = join_by(word)) |&gt;\n#   slice(1:30)\n#   # NOTE: checked slices 1:30\n\n\n# Using this method to look at the text to determine if I should remove the word or not\n# or us a regex method\n# fitzgerald |&gt;\n#   filter(str_detect(text, \"gentlemen\"))\n\n\n# going to get rid of some words here\n# dropwords = c(\"majesty\", \"prisoner\", \"master\", \"excellency\")\n\n\n# # author sentiment\n# fitzgeraldsentiment = tidy_fitzgerald |&gt; \n#   # get rid of drop words\n#   # filter(!word %in% dropwords) |&gt; \n#   # join with bing to get sentiment\n#   inner_join(bing, by = join_by(word)) |&gt; \n#   # TODO: what is this 80 for?\n#   count(title, page = linenumber %/% 80, sentiment) |&gt; \n#   # pivot wider data here\n#   pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n#   # get sentiment score\n#   mutate(sentiment = positive - negative) \n# # check head of data\n# head(fitzgeraldsentiment)\n# # NOTE: looks good!\n# \n# \n# # ggplot of basic analysis of sentiment overtime\n# ggplot(fitzgeraldsentiment, aes(page, sentiment, fill = title)) + \n#   # make bar graph, remove legend cause dont need\n#   geom_bar(stat = \"identity\", show.legend = FALSE) + \n#   # plot by book/title\n#   facet_wrap(~title, ncol = 3, scales = \"free_x\") \n# \n# \n# # plot the data\n# g = fitzgeraldsentiment|&gt; \n#   # group by title\n#   group_by(title) |&gt; \n#   # get cumulative sentiment over time\n#   mutate(sentiment = cumsum(sentiment), page = page/max(page)) |&gt; \n#   # plot sentiment over time\n#   ggplot(aes(page, sentiment, colour = title)) + \n#   # make the line width bigger\n#   geom_line(linewidth = 1.25) + \n#   # labels\n#   labs(\n#     x = \"Percent of Total Pages (%)\",\n#     y = \"Cumulative Sentiment\",\n#     title = \"Trajectory of Sentiment Throughout Fitzgerald's books\",\n#     caption = \"Data Source: Project Gutenberg\"\n#   )\n# \n# # making transparent legend\n# transparent_legend = theme(legend.background = element_rect(fill = \"transparent\"), legend.key = \n#                              element_rect(fill = \"transparent\", color = \"transparent\"))\n# \n# # plot\n# g + \n#   # add transparent legend\n#   transparent_legend + \n#   # change colors\n#   scale_color_brewer(type = \"qual\") +\n#   # scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12), breaks = desired_order) +\n#   # make specific colors go to specific titles\n#   # scale_colour_manual(\n#   #   values = setNames(desired_colors, desired_order),\n#   #   breaks = desired_order\n#   # ) +\n#   # change x axis to percent\n#   scale_x_continuous(labels = scales::percent_format()) + \n#   # change theme to classic\n#   theme_classic() + \n#   # edit text\n#   theme(\n#     legend.position = c(0.2, 0.25), \n#     text = element_text(size = 12),\n#     plot.title = element_text(face = \"bold\", hjust = 0.5),\n#     legend.text = element_text(face = \"italic\")\n#     ) + \n#   # change legend postion\n#   guides(colour = guide_legend(title = \"Book\", override.aes = list(linewidth = 2)))\n\n\nUltimately, the purpose of this analysis was to: 1. Differentiate between the three authors using relevant keywords when excluding names and proper nouns 2. Identify interesting patterns among authors and/or books.\nFor these reasons, certain words identified in earlier analyses were excluded from the current results presented. These excluded words included names within the books such as “anthony,” “gatsby,” or “dantès,” as well as common pronouns like “sir,” “madame,” and “dear.” Additionally, words such as “lord,” “queen,” and “monk” were removed because within the context of the data they typically functioned more as nouns referring to individuals.\nSee the “Code” section below for the full list of words excluded from the analysis.\n\n\nCode\n# Coming back and getting rid of words (mainly names)\nwords_i_dont_want = c(\"anthony\", \"gloria\", \"amory\", \"aramis\", \"porthos\",\"athos\",\"d’artagnan\",\"count\",\n                      \"de\", \"monte\",\"cristo\",\"villefort\",\"danglars\",\"madame\", \"morrel\", \"cornelius\", \"rosa\",\n                      \"monsieur\",\"dantès\", \"valentine\", \"franz\", \"sir\", \"friend\", \"albert\",\"girl\", \"king\",\n                      \"lord\",\"queen\",\"raoul\",\"mazarin\",\"father\", \"caderousse\", \"sire\", \"morcerf\",\"majesty\",\n                      \"milady\", \"friends\", \"cardinal\", \"loius\", \"monk\", \"colbert\", \"fouquet\", \"dear\",\"daisy\",\n                      \"tom\", \"gatsby\", \"grimaud\", \"planchet\", \"la\", \"tulip\", \"louis\", \"prince\", \"woman\",\n                      \"duke\", \"mordaunt\", \"paris\", \"gentlemen\", \"boxtel\", \"baerle\",\"rosalind\", \"gryphus\",\"maury\",\n                      \"charles\", \"le\", \"francs\", \"buckingham\",\"comte\", \"guiche\",\"edmond\", \"andrea\",\"noirtier\",\n                      \"malicorne\", \"poet\", \"ii\", \"baisemeaux\", \"montalais\", \"bonacieux\", \"chapter\",\"prisoner\")\n\n\nLDA was first performed with three topics, with results and thoughts below.\n\n\n\nCode\n### LDA Analysis of all Authors ###\n\n\n# Get bag of words\n# author 1 bow\ntidy_freq_dumas = tidy_dumas  |&gt; \n  dplyr::ungroup()  |&gt; \n  # count words\n  count(title, word, name = \"count\") |&gt; \n  # filter for numbers\n  filter(is.na(as.numeric(word))) |&gt; \n  # get rid of this novel\n  filter(title != \"The Wolf-Leader\") |&gt; \n  # retroactively get rid of these words\n  filter(!word %in% words_i_dont_want)\n\n\n# author 2 bow\ntidy_freq_aristotle = tidy_aristotle  |&gt; \n  dplyr::ungroup()  |&gt; \n  # count words\n  count(title, word, name = \"count\") |&gt; \n  # filter for numbers\n  filter(is.na(as.numeric(word))) |&gt; \n  # retroactively get rid of these words\n  filter(!word %in% words_i_dont_want)\n\n\n# author 1 bow\ntidy_freq_fitzgerald = tidy_fitzgerald  |&gt; \n  dplyr::ungroup()  |&gt; \n  # count words\n  count(title, word, name = \"count\") |&gt; \n  # filter for numbers\n  filter(is.na(as.numeric(word))) |&gt; \n  # retroactively get rid of these words\n  filter(!word %in% words_i_dont_want)\n\n\n# combine data\ndf_authors123 = rbind(tidy_freq_dumas, tidy_freq_aristotle, tidy_freq_fitzgerald) |&gt; \n  # get rid of stop words\n  anti_join(stop_words, by = \"word\") |&gt; \n  # arrange in descending order\n  arrange(desc(count))\n# head(df_authors123)\n\n\n# make Document Term Matrix\ndtm_author &lt;- df_authors123  |&gt; \n  cast_dtm(title, word, count)\n\n\n# Perform LDA on 3 topics\nlda_author &lt;- LDA(dtm_author, k = 3L, control = list(seed = 10))\n# lda_author\n\n\n# Look at words per topic\nbeta_author &lt;- tidy(lda_author, matrix = \"beta\")\n# beta_author\n\n\n# look at top terms\ntop_terms &lt;- beta_author  |&gt; \n  # group by topic\n  group_by(topic)  |&gt; \n  # show top 10\n  slice_max(beta, n = 15)  |&gt;  \n  ungroup()  |&gt; \n  # arrange by lowest beta\n  arrange(topic, -beta)\n# top_terms\n\n\n# plot top terms\ntop_terms |&gt; \n  # reorder terms based on beta and topic\n  mutate(term = reorder_within(term, beta, topic)) |&gt;\n  # change topic from numbers to legible lables\n  mutate(topic = case_when(topic == 1 ~ \"Topic 1\",\n                           topic == 2 ~ \"Topic 2\",\n                           topic == 3 ~ \"Topic 3\")) |&gt; \n  # begin plotting\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  # columns, or bars for expressing data\n  geom_col(show.legend = FALSE) +\n  # wrao based on the topic\n  facet_wrap(~ topic, scales = \"free_y\", nrow=2, ncol=2) +\n  scale_y_reordered() +\n  # labels for plot\n  labs(\n    x = \"Word Probability Per Topic (\\u03B2)\",\n    y = \"Word\",\n    title = \"Probability of Top Words Per Topic From a 3-Topic LDA\",\n    caption = \"Data Source: Project Gutenberg\"\n  ) + \n  # change theme\n  theme_linedraw() +\n  # edit text and legend\n  theme(\n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    legend.text = element_text(face = \"italic\")\n    ) +  \n  # change colors of plot\n    scale_fill_manual(\n    values = setNames(c(\"darkred\", \"darkgreen\",\"darkblue\"), c(\"Topic 1\", \"Topic 2\", \"Topic 3\")))\n\n\n\n\n\nExpressed above are the top 15 words within each topic determined by a 3-Topic LDA. Note there are no names, pronouns, or other words we omitted from the analysis. Topic 1 appears to be influenced by Aristotle’s books, as it contains words like “government,” “power,” and “voice.” Topic 2, and to a lesser degree Topic 3, contain many words in the past tense such as “replied” or “cried.” This interesting find suggests that some authors may have a preference for using the past tense more frequently than others.\nThere are some overlaps of words between topics such as “eyes” or “time.” In future work, particularly when classifying books, it might be of interest to examine why these overlaps are present and omit them from the analysis. However, for the purpose of this study we interpret these overlaps as potential commonalities between authors’ works and possible differences in word usage given specific contexts. For instance, in Dumas’ The Black Tulip, example quotes such as ““Of a tumult?” replied Cornelius, fixing his eyes on his perplexed” and “John, with tears in his eyes, wiped off a drop of the noble blood” showcase Dumas’ use of the word “eye” to describe eye actions. Conversely, in Aristotle’s Politics: A Treatise on Government, examples like “can see better with two eyes, and hear better with two ears” and “see that absolute monarchs now furnish themselves with many eyes” demonstrate Aristotle’s usage of “eye” as a noun rather than describing its actions. Countless more examples can be found in the text of words such as this.\nIt is worth noting that a word like “replied” likely differs from the “eye” example, as it is predominantly used when a person is responding to someone else. This common term is used frequently in books, so it was certainly interesting to see how it varies across topic and books as shown in the figure above and below.\n\n\nCode\n### LDA Analysis of all Authors Gamma Plot ###\n\n# check doc in each topic\ngamma_author &lt;- tidy(lda_author, matrix = \"gamma\")\n# gamma_author\n\n\n# get titles for each other\na1_titles = unique(tidy_dumas$title)\na2_titles = unique(tidy_aristotle$title)\na3_titles = unique(tidy_fitzgerald$title)\n# order for plot\nplot_order = c(a1_titles, a2_titles, a3_titles)\n\n\n# plot!\ngamma_author  |&gt; \n  # make title as factor of document, get plot order correct\n  mutate(title = factor(document, levels = plot_order))  |&gt; \n  # make new column author to use for facet wrap for more legible plot\n  mutate(author =  case_when(title %in% a1_titles ~ \"Alexandre Dumas\",\n                             title %in% a2_titles ~ \"Aristotle\",\n                             title %in% a3_titles ~ \"Scott F. Fitzgerald\")) |&gt; \n  # begin plot\n  ggplot(aes(x = title, y = gamma, fill = factor(topic))) +\n  # facet wrao by author with same y\n  facet_wrap(~author, scales = \"free_x\") +\n  # barplots\n  geom_col(width = 0.8) +\n  # labels\n  labs(\n    x = \"Book\",\n    y = paste(\"Topic Probability Per Book (\\u03B3)\"),\n    title = \"Proportion of Topics Per Book From a 3-Topic LDA\",\n    caption = \"Data Source: Project Gutenberg\",\n    fill = \"Topic\"\n  ) +\n  # change theme\n  theme_linedraw() +\n  # edit text and legend\n  theme(axis.text.x = element_text(angle = 55, hjust = 1),\n        plot.title = element_text(face = \"bold\", hjust = 0.5),\n        # legend.position = c(0.25, 0.3), \n        text = element_text(size = 12)) + \n  # change colors of fill\n  scale_x_discrete(labels = function(y) str_wrap(y, width = 20), expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"darkred\", \"darkgreen\",\"darkblue\")) +\n  # scale_fill_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # add some padding\n  coord_cartesian(xlim = c(0.5,  5 + 0.5), expand = FALSE)\n\n\n\n\n\nThe figure above expresses how each topic was represented in the various books and highlights some differences between the authors. After removing names and pronouns, we see that each topic does not correspond exclusively to each author. However, the figure does reveal some intriguing results. Fitzgerald’s books exclusively consist only of Topic 1, Dumas’ books heavily feature Topic 2, while Aristotle’s works are a mixture of both Topics 1 and 2. This suggests how Fitzgerald and Aristotle may have more similar writing styles, or at least use more words in common, compared to Dumas. It also suggests that Dumas and Fitzgerald have little similarities in their writing style and choice of words.\nInterestingly, Topic 3 resides almost only in Dumas’ The Count of Monte Cristo, Illustrated. The unique words of this topic, such as “return” and “heard,” differ entirely from the top 15 words of the other topics. This also suggests that words such as “door” and “house” are used more frequently in this specific book compared to others. Additionally, a bit of Topic 3 was located in three other books written by Dumas emphasizing some consistency within his writing.\nUpon further examination of the data, it was discovered that Aristotle never mentions the word “cried” in any of his five works analyzed. However, Topic 2, which has the term “cried” as its third highest coefficient, represents a majority of two of Aristotle’s books. This suggests that including more topics will likely enhance our understanding of the underlying connections of these authors and their works. For that reason, another LDA was conducted using five topics. Note that Topics 1-3 will not be identical to the previous analysis.\n\n\n\nCode\n### LDA with more than 3 topics ###\n\n\n# Perform LDA on 3 topics\nlda_author &lt;- LDA(dtm_author, k = 5L, control = list(seed = 10))\n# lda_author\n\n\n# Look at words per topic\nbeta_author &lt;- tidy(lda_author, matrix = \"beta\")\n# beta_author\n\n\n# look at top terms\ntop_terms &lt;- beta_author  |&gt; \n  # group by topic\n  group_by(topic)  |&gt; \n  # show top 10\n  slice_max(beta, n = 15)  |&gt;  \n  ungroup()  |&gt; \n  # arrange by lowest beta\n  arrange(topic, -beta)\n# top_terms\n\n\n\n# plot top terms\ntop_terms |&gt; \n  # reorder terms based on beta and topic\n  mutate(term = reorder_within(term, beta, topic)) |&gt;\n  # change topic from numbers to legible lables\n  mutate(topic = case_when(topic == 1 ~ \"Topic 1\",\n                           topic == 2 ~ \"Topic 2\",\n                           topic == 3 ~ \"Topic 3\",\n                           topic == 4 ~ \"Topic 4\",\n                           topic == 5 ~ \"Topic 5\")) |&gt; \n  # begin plotting\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  # columns, or bars for expressing data\n  geom_col(show.legend = FALSE) +\n  # wrao based on the topic\n  facet_wrap(~ topic, scales = \"free_y\", nrow=2, ncol=3) +\n  scale_y_reordered() +\n  # labels for plot\n  labs(\n    x = \"Word Probability Per Topic (\\u03B2)\",\n    y = \"Word\",\n    title = \"Probability of Top Words Per Topic From a 5-Topic LDA\",\n    caption = \"Data Source: Project Gutenberg\"\n  ) + \n  # change theme\n  theme_linedraw() +\n  # edit text and legend\n  theme(\n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    legend.text = element_text(face = \"italic\"),\n    axis.text.x = element_text(size = 8)\n    ) +  \n  # change colors of plot\n    scale_fill_manual(\n    values = setNames(c(\"darkred\", \"darkgreen\",\"darkblue\", \"darkorange\", \"#4B0076\"), c(\"Topic 1\", \"Topic 2\", \"Topic 3\", \"Topic 4\", \"Topic 5\")))\n\n\n\n\n\nA total of five topics were chosen in attempt to understand the differences between authors and their respective books. The previous analysis, using three topics, had shown to encounter an issue where certain topic words did not align with the books they were associated with. Although topic values ranging from four to eight were explored, it was determined that five topics was the most interesting and worth investigating further.\nThe above figure showcases the top 15 words for each of the newly generated topics produced from the LDA analysis. Topic 1 displays a strong correlation with Aristotle’s works on politics and philosophy, evident through words such as “government,” “public,” and “law.” Topics 2 and 3 contain many similar words like “time” and “cried” which were predominately related to Dumas’ books in the previous analysis. Notably, Topic 2 introduces the word “whilst,” while Topic 3 introduces the word “honor” which serve as novel distinguishing terms for differentiating between authors. In Topic 4, the word “eyes” has the highest score along with words like “night” and “day” that seemed to be connected to Fitzgerald in the last analysis. This topic also introduced the words “suddenly” and “love.” Lastly, Topic 5 was nearly identical to Topic 3 in the previous analysis, which primarily consisted of Dumas’ The Count of Monte Cristo, Illustrated.\nTo reemphasize, although there are many word overlaps across topics they were retained in hope to better understand the interaction of words within each topic. One interesting word that may pique the readers’ curiosity is “ah,” which has been retained in the analysis. This word was exclusively found in the works of Alexandre Dumas, as shown in the figures above. It is written in the text as instances like ““Ah! ah!” within twelve hours, you say?” and ““Ah, ah!” said William to his dog, “it’s easy to see that she is a”” which are from The Black Tulip. The decision was made not to omit this word, as it was used more frequently during this time period and aids in distinguishing between authors.\n\n\nCode\n### LDA Analysis of all Authors Gamma Plot ###\n\n# check doc in each topic\ngamma_author &lt;- tidy(lda_author, matrix = \"gamma\")\n# gamma_author\n\n\n# plot!\ngamma_author  |&gt; \n  # make title as factor of document, get plot order correct\n  mutate(title = factor(document, levels = plot_order))  |&gt; \n  # make new column author to use for facet wrap for more legible plot\n  mutate(author =  case_when(title %in% a1_titles ~ \"Alexandre Dumas\",\n                             title %in% a2_titles ~ \"Aristotle\",\n                             title %in% a3_titles ~ \"Scott F. Fitzgerald\")) |&gt; \n  # begin plot\n  ggplot(aes(x = title, y = gamma, fill = factor(topic))) +\n  # facet wrao by author with same y\n  facet_wrap(~author, scales = \"free_x\") +\n  # barplots\n  geom_col(width = 0.8) +\n  # labels\n  labs(\n    x = \"Book\",\n    y = paste(\"Topic Probability Per Book (\\u03B3)\"),\n    title = \"Proportion of Topics Per Book From a 5-Topic LDA\",\n    caption = \"Data Source: Project Gutenberg\",\n    fill = \"Topic\"\n  ) +\n  # change theme\n  theme_linedraw() +\n  # edit text and legend\n  theme(axis.text.x = element_text(angle = 55, hjust = 1),\n        plot.title = element_text(face = \"bold\", hjust = 0.5),\n        # legend.position = c(0.25, 0.3), \n        text = element_text(size = 12)) + \n  # change colors of fill\n  scale_x_discrete(labels = function(y) str_wrap(y, width = 20), expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"darkred\", \"darkgreen\",\"darkblue\", \"darkorange\", \"#4B0076\")) +\n  # scale_fill_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # add some padding\n  coord_cartesian(xlim = c(0.5,  5 + 0.5), expand = FALSE)\n\n\n\n\n\nThe final figure above illustrates the representation of each topic across the various books while highlighting certain differences among the authors. When the number of topics was increased from three to five there was a complete separation among all authors, with Dumas exhibiting subcategories within his books.\nUnsurprisingly, Aristotle was exclusively represented by Topic 1, which was not found in any other novel. This phenomena likely occured because of words within the top 15 of Topic 1 such as “government,” “democracy,” and “oligarchy,” which closely resemble the themes explored in Aristotle’s works on politics. It was fascinating that these words primarily relate to Aristotle’s Politics: A Treatise on Government and The Athenian Constitution rather than his poetic works of The Poetics of Aristotle and Aristotle on the art of poetry. Further exploration may involve identifying distinct keywords that better differentiate these bodies of work from one another.\nFitzgerald was represented solely by Topic 4 in this analysis, which was characterized by words such as “night,” “day,” “suddenly,” and “love.” Considering the selected works of this author, it comes as no surprise that these words achieve high scores for Fitzgerald. However, if words such as “woman,” “girl,” or “gentlemen,” which were excluded from the analysis, were included, the books would likely better differentiate into different topics.\nMost interestingly, despite being compared to Aristotle and Fitzgerald’s works, Dumas’ five works are divided into three topics. The Count of Monte Cristo, Illustrated forms its own distinctive topic, Topic 5, corresponding exactly to Topic 3 in the previous analysis. Topic 3 was mainly found in The Three Musketeers and Ten Years Later, while Topic 4 was predominately in The Black Tulip and Twenty Years After. It was expected that the D’Artagnan Romances trilogy would fall under a single topic, and while this was mostly the case, Twenty Years After contains a significant portion of Topic 2. Topic 2 was fully associated with The Black Tulip, an adventure novel by Dumas characterized with distinguishing words such as “whilst” and “van.” Overall, it was satisfying to observe that this analysis successfully achieves its objective of distinguishing between authors and their works, while also revealing aforementioned subcategories within Dumas’ books."
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html#conclusion",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html#conclusion",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the authors’ works exhibit notable differences as revealed through this analysis. Specific words unique to each author were successfully identified, indicating distinct writing styles among Dumas, Aristotle, and Fitzgerald. Moreover, the analysis successfully distinguished all three authors and discovered the previously mentioned subcategories within Dumas’ works using a five-topic LDA model. Future research could focus on identifying distinguishing words between books authored by Aristotle and Fitzgerald."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Performing Principal Component Analysis on Flag Images in R\n\n\n\n\n\n\n\nproject\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nCaleb Hallinan\n\n\n\n\n\n\n  \n\n\n\n\nSentiment and Topic Model Analysis of Alexandre Dumas & Others\n\n\n\n\n\n\n\nproject\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nCaleb Hallinan\n\n\n\n\n\n\n  \n\n\n\n\nMight try blogging…\n\n\n\n\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nCaleb Hallinan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software and data packages",
    "section": "",
    "text": "TBD\n\n\n\nTBD"
  },
  {
    "objectID": "software.html#bioconductor",
    "href": "software.html#bioconductor",
    "title": "Software and data packages",
    "section": "",
    "text": "TBD\n\n\n\nTBD"
  },
  {
    "objectID": "software.html#cran",
    "href": "software.html#cran",
    "title": "Software and data packages",
    "section": "CRAN",
    "text": "CRAN\n\nSoftware packages\nTBD"
  },
  {
    "objectID": "software.html#python",
    "href": "software.html#python",
    "title": "Software and data packages",
    "section": "Python",
    "text": "Python\n\nSoftware packages\nTBD"
  },
  {
    "objectID": "software.html#github",
    "href": "software.html#github",
    "title": "Software and data packages",
    "section": "GitHub",
    "text": "GitHub\n\nSoftware packages\nTBD\n\n\nData packages\nTBD"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Caleb Hallinan",
    "section": "",
    "text": "First Year PhD Student in Biomedical Engineering at Johns Hopkins University \nUltimately, my hope is to develop user-friendly computational software specifically tailored for biologists who may not be as tech-savvy. I was recently introduced to spatial transcriptomics (ST) and immediately captivated, so I am thrilled to explore this field during my rotation with Dr. Fan in the JEFworks Lab! My proficiencies lie in the domains of data analysis, machine learning, and the development and implementation of high-end computational research tools.\nWhen I’m not busy with research, you’ll find me playing/watching sports, hanging out with friends, or watching movies :) I also have a passion for teaching and mentoring, so I am looking forward to pursing this further at Hopkins as well!\n\n\nJohns Hopkins University, Baltimore, MD\nPhD in Biomedical Engineering | Aug 2023 - TBD 2028\nUniversity of Virginia | Charlottesville, VA\nB.A. in Statistics and Biology | Aug 2017 - May 2021\n\n\n\nBoston Children’s Hospital | Boston, MA\nResearch Assistant | Sept 2021 - June 2023"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Caleb Hallinan",
    "section": "",
    "text": "Johns Hopkins University, Baltimore, MD\nPhD in Biomedical Engineering | Aug 2023 - TBD 2028\nUniversity of Virginia | Charlottesville, VA\nB.A. in Statistics and Biology | Aug 2017 - May 2021"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Caleb Hallinan",
    "section": "",
    "text": "Boston Children’s Hospital | Boston, MA\nResearch Assistant | Sept 2021 - June 2023"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "I am currently rotating in labs at JHU to determine what I’ll work on next!"
  },
  {
    "objectID": "projects.html#new-projects",
    "href": "projects.html#new-projects",
    "title": "Projects",
    "section": "",
    "text": "I am currently rotating in labs at JHU to determine what I’ll work on next!"
  },
  {
    "objectID": "projects.html#subtype-discovery-method-utilizing-scrna-seq-and-microarray-data",
    "href": "projects.html#subtype-discovery-method-utilizing-scrna-seq-and-microarray-data",
    "title": "Projects",
    "section": "subtype discovery method utilizing scRNA-seq and microarray data",
    "text": "subtype discovery method utilizing scRNA-seq and microarray data\n\nOur method, PHet, is able to distinguish multiple subtypes of data given only two labels (control and case)\n\n\n\n\nSummary of Abstract\nIn disease diagnosis and targeted therapy, discovering subtypes is crucial as cells or patients can exhibit varied responses to treatments. Hence, understanding the heterogeneity of disease states is vital for comprehending pathological processes. However, selecting features for subtyping from high-dimensional datasets is challenging, with many algorithms focusing on known disease phenotypes and potentially overlooking valuable subtyping information. Our study aimed to address this issue by identifying feature sets that preserve heterogeneity while discriminating known disease states. Through a data-driven approach combining feature clustering and deep metric learning, we developed a statistical method called PHet (Preserving Heterogeneity). This method effectively identifies a minimal set of features that maintain heterogeneity while maximizing the quality of subtype clustering. PHet outperformed previous methods in identifying disease subtypes using microarray and single-cell RNA-seq datasets. Our research provides an innovative feature selection method that facilitates personalized medicine and enhances understanding of disease heterogeneity. I am co-first author with my former labmate Dr. Abdurrahman Abul-Basher.\nCheck out the preprint here."
  },
  {
    "objectID": "projects.html#image-analysis-of-live-cell-imaging",
    "href": "projects.html#image-analysis-of-live-cell-imaging",
    "title": "Projects",
    "section": "image analysis of live-cell imaging",
    "text": "image analysis of live-cell imaging\nI worked under Dr. Kwonmoo Lee at Boston Children’s Hospital as a Research Assistant for two years after undergrad. My work included using various Convolutional Neural Networks (CNNs) for cell segmentation, utilizing cell tracking algorithms, honing my image manipulation skills (Fiji), working on my research writing aptitude, and much more! It was an incredible experience that has led me to where I am now :) Below are two papers I had the privilege of working on:\n\nThe Lee Lab developed a deep learning-based pipeline termed MARS-Net . While I did not contribute to it’s development, I helped the first author write the protocol for running it here.\nI helped in optimizing the hyper parameters for the R-CNN in FNA-Net, a deep-learning based ensemble model aimed to screen the adequacy of unstained thyroid fine needle aspirations (FNA). Ideally, this will streamline the diagnostic process by eliminating the need for staining and expert interpretation.\nMy biggest project was a subtype discovery method termed PHet. The abstract and preprint are above!"
  },
  {
    "objectID": "projects.html#github-contributions",
    "href": "projects.html#github-contributions",
    "title": "Projects",
    "section": "github contributions",
    "text": "github contributions"
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html",
    "href": "blog/2023-10-20-pca-on-images/index.html",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "",
    "text": "In this project, we will embark on an adventure using Principal Component Analysis on images of various flags of countries/regions. Hopefully by the end you will have a better understanding of PCA and how we can utilize it to look at images in R!\nNote: I condensed the R code to “Code” sections throughout for scrolling purposes. Feel free to click on these sections to look at each step, or change to “Show All Code” in the top right of the webpage!"
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#introduction",
    "href": "blog/2023-10-20-pca-on-images/index.html#introduction",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "Introduction",
    "text": "Introduction\nPrincipal Components Analysis (PCA) is a well-established technique to reduce the dimensions or features of a dataset while preserving the maximum amount of variation. This technique is widely used for pattern recognition, signal processing, and machine learning. PCA is also a useful tool in denoising, visualization, and classification of large datasets given features are linearly related. For example, imagine you have a genomics dataset with 10000+ genes and 5000+ observations. That’s a lot of data! However, not all of those genes will be important as likely many of them are correlated, or express similar variation, as others. PCA is a great tool to reduce the 10000+ dimensions to a smaller number, eliminating redundancy by reducing the dimensions of the dataset for more downstream analysis.\n\n\n\n\n\nImage Source\n\n\n\nPCA is heavily based on linear algebra concepts. Essentially, PCA computes the eigenvectors of the covariance matrix of the data and sorts them by their eigenvalue (which correspond to the explained variance for that individual principal component). The principal components (PC) are then computed as linear combinations of the original variables using the eigenvectors. It sounds complicated, but I promise you it’s not as hard to understand as you think. This article was a great resource for me when I was first trying to figure it out. Unfortunately, I won’t go into much of the algebra behind PCA, so I encourage you to research it more! You can see this really cool animation above expressing what PCA is doing in two dimensions. Briefly, it is finding the direction that maximizes the variance of the blue dots which can also be viewed as minimizing the residuals of the blue dots to a line."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#data",
    "href": "blog/2023-10-20-pca-on-images/index.html#data",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "Data",
    "text": "Data\nIn this project specifically, instead of working with gene expressions or large datasets with numerous features, we are looking at images of flags from different countries. The goal is to apply PCA to these flags to assess its effectiveness in preserving variation and possibly discover interesting patterns or insights in the principal components themselves.\nTo begin our exploration, we need to install and load some R packages. These packages will equip us with the necessary tools for working with data frames, functions, file paths, reading images, and image processing.\n\n\n\nCode\n### install and read in packages\n\nlibrary(tidyverse) # for dataframes\nlibrary(here) # for use of others if downloaded\nlibrary(png) # for reading png images\nlibrary(magick) # for reading in images\nlibrary(imager) # for plotting\n\n\nNow that we have the necessary packages, let’s grab the flag data from here. This dataset contains flags of varying sizes within different folders, however we are going to look specifically at the “/png250px/” folder. This folder has 255 flags from various countries and regions.\n\n\n\nCode\n### Grabbing the data\n\n# url to flag data, it is in zip file\nurl = \"https://github.com/hampusborgos/country-flags/archive/refs/heads/main.zip\"\n\n# specify the file name and location where you want to save the file on your computer\nfile_name = \"flags.zip\"\nfile_path = here()\n\n# use the download.file() function\ndownload.file(url, paste(file_path, file_name, sep = \"/\"), mode = \"wb\")\n\n# unzip zip file\nunzip(paste0(here(), \"/flags.zip\"), exdir = here())\n\n# get file names\nfiles = list.files(here(\"country-flags-main/png250px/\"), full.names = TRUE)\n\n# Read each image file in the folder\nimage_list = lapply(files, image_read)\n# image_list = lapply(files, readPNG)\n\n\nFor PCA to work soundly, it is essential that the dataset’s dimensions remain consistent. Of course, that’s not the case in this dataset where we see all flags have the same height (250px) but very different widths. Hence, we need to resize each flag to be the same height and width. I use the magick R package to read in the images and the “image_scale” along with the “image_convert” functions to transform the images into size 250x250x3 (representing height, width, and color channel). These resized images are then saved in the folder “/resized_png250px/.” To enable PCA analysis, the images are converted from matrix to vector format. This involves flattening the 250x250x3 image matrix into a single vector of size 1x187500 (250 x 250 x 3). The individual vectors for each image are then combined into a single matrix of dimensions 255x187500. This variable, “image_matrix” is created and saved as flags_matrix.RDS. Also to note, this matrix is country/region flag image x pixel of flag image.\n\n\nCode\n# NOTE: so each image is the same height but very different widths lol. Let's change that\n\n# Set height and width I want\nmax_height = 250\nmax_width = 250\n\n# Resize all the images to the specified height and width\n# had to add matte=FALSE to get rid of extra channel\nresized_images = lapply(image_list, function(im) {\n  image_convert(image_scale(im, paste0(max_width, \"x\", max_height, \"!\")), format = \"png\", matte=FALSE)\n})\n\n\n# Create the directory to save the resized images\ndir.create(\"resized_png250px\")\n\n# Save the resized images with the same names as the original files\nfor (i in seq_along(resized_images)) {\n  # Extract the file name from the full path\n  file_name = basename(files[i])\n  # make the file path for saving the resized image\n  save_path = file.path(\"resized_png250px\", file_name)\n  # Write the resized image to the specified file path\n  image_write(resized_images[[i]], save_path)\n}\n\n\n# now grab them with png package\nresized_files = list.files(here(\"blog/2023-10-20-pca-on-images/resized_png250px/\"), full.names = TRUE)\n\n# use function readPNG\nimgs_final = lapply(resized_files, readPNG)\n\n# QC: check each image is same dimensions\n# for (i in seq_along(imgs_final)) {\n#   dimensions = dim(imgs_final[[i]])\n#   cat(\"Image\", i, \"Dimensions:\", dimensions[1], \"x\", dimensions[2], \"x\", dimensions[3], \"\\n\")\n# }\n\n# great!\n\n# Get the number of images in the list\nnum_images = length(image_list)\n\n# Create an empty matrix to store the flattened images\nimage_matrix = matrix(NA, nrow = num_images, ncol = 250 * 250 * 3)\n\n# Flatten each image and store it as a column in the matrix\nfor (i in 1:num_images) {\n  # get the flatten vector\n  flattened_image = as.vector(imgs_final[[i]])\n  # add to matrix\n  image_matrix[i,] = flattened_image\n}\n\n# save as .rds file\n# saveRDS(image_matrix, file = here(\"flags_matrix.RDS\"))\n\n\n# plot images\n\n# want to add all images to this dataframe for ggplot\nall_images = data.frame()\n\n# for loop getting 5 examples\nfor (i in seq(1,190,20)) {\n  # getting image - making cimg from imager package and then df\n  img = as.data.frame(as.cimg(imgs_final[[i]]), wide = \"c\") |&gt; \n    # # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # # make class labels\n    mutate(img_num = paste(\"Image\",i))\n  \n  all_images = rbind(all_images, img)\n  \n  # make the levels of img variable what i want\n  all_images$img_num = factor(all_images$img_num, levels = sprintf(\"Image %d\", seq(1, 190, 20)))\n\n}\n\n# plot image\nprint(ggplot(all_images,aes(y,x))+\n  # getting rgb\n  geom_raster(aes(fill=rgb_value))+\n  # fill image\n  scale_fill_identity() +\n  # facet_wrap\n  facet_wrap(.~ img_num, nrow = 2, ncol = 5) +\n  # reverse y axis\n  scale_y_reverse() + \n  # get rid of theme\n  theme_void() +\n  # bigger font size\n  theme(strip.text = element_text(size = 12)))\n\n\n\n\n\n\n\n\n\nHere are ten examples of the flag image data we are using."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#pca-on-the-training-data",
    "href": "blog/2023-10-20-pca-on-images/index.html#pca-on-the-training-data",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "PCA on the training data",
    "text": "PCA on the training data\nTechnically our “image_matrix” variable could be input straight into PCA the way it is. However, we are going to split the 255 images into a training and test set. Why do we do this? Well, for one it was part of the project description 😅 But really we do this to see how well PCA will predict, or project, the test data using only information from the training data. Our training set we take to be 75% of the data (191 flags) leaving 25% of data being the test images (64 flags). Finally, we are ready to conduct PCA on the training data! Note that we are setting center=TRUE and scale.=False. Briefly, the center hyperparameter shifts the data to be zero centered (subtracting the mean from each column) while scale will make the data have unit variance (correlation instead of covariance PCA). Centering is crucial for PCA to perform correctly, however scaling is dataset dependent. Take a look at the prcomp function description for more information.\n\n\n\nCode\n### Do PCA analysis ###\n\n# Set the seed \nset.seed(123)\n\n# Get the number of images in the list - for some reason needed to add this to this code chunk\nnum_images = dim(image_matrix)[1]\n\n# Calculate the number of columns for training data\ntrain_columns = floor(0.75 * num_images)\n\n# Randomly select indices for the training data\ntrain_indices = sample(1:num_images, train_columns)\n\n# Get the test indices\ntest_indices = setdiff(1:num_images, train_indices)\n\n# Split the image matrix into training and test data\ntrain_data = image_matrix[train_indices, ]\ntest_data = image_matrix[-train_indices, ]\n\n# Perform PCA on the training data, centering data but NOT scaling\npca_result = prcomp(train_data, center=TRUE, scale. = FALSE)\n\n\nLet’s extract some key information form the prcomp function. We are able to calculate the proportion of variance explained from each PC by looking at the \\(sdev\\) value, aka the standard deviations of the principal components, by squaring them and dividing by the sum of the squared \\(sdev\\) values. To note, \\(sdev^2\\) is actually equal to the eigenvalues of the dataset! We can then get the cumulative variation as the number of PC components increase, leading to the plot you see below. Here we see that as PCs increase so does the cumulative variance preserved/explained. This is an excellent visualization to check how much each PC contributes to preserving overall variation. In our case we see that with just 67 PCs we can explain 95% of the variation in the dataset!\n\n\nCode\n# Extract the proportion of variance explained by each principal component\nvariance_explained = pca_result$sdev^2 / sum(pca_result$sdev^2)\n\n# Calculate the cumulative percentage of variance explained\ncumulative_variance = cumsum(variance_explained) * 100\n\n# get pcs getting more than 95% of the data\npcs_for_95 = which(cumsum(variance_explained) &gt;= 0.95)[1]\n\n# Create a tibble for plotting\ndata_plot = tibble(\n  # x axis\n  num_components = 1:length(cumulative_variance),\n  # cumvar\n  cumulative_variance = cumulative_variance\n)\n\n# plot using ggplot2\nggplot(data_plot, aes(x = num_components, y = cumulative_variance)) +\n  # make line plot\n  geom_line() +\n  # add points\n  geom_point() +\n  # adding line for which PC number we are getting\n  geom_vline(xintercept = pcs_for_95, color = \"red\", linetype = \"dashed\") +\n  # adding text for PC im getting\n  annotate(\"text\", x = 70, y = 85, label = paste(\"95% variation explained\\nwith\", as.character(pcs_for_95), \"PCs\"), hjust = 0, vjust = 0, color = \"black\", size = 4) +\n  # x axis every 20\n  scale_x_continuous(breaks = seq(0, max(data_plot$num_components), by = 20)) +\n  # y axis every 20\n  scale_y_continuous(breaks = seq(0, 100, by = 20)) +\n  # labels\n  labs(x = \"Number of Principal Components\",\n       y = \"Cumulative Variance Explained (%)\",\n       title = \"Cumulative Variance Explained with Additional Principal Components\") +\n  # theme\n  theme_bw() +\n  # change text vars\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        axis.text = element_text(size = 10))\n\n\n\n\n\n\n\n\n\nAnother popular plot used to visualize PCs and their explained variation is a scree plot. Check out the wiki page for more information.\nSo we were able to successfully perform PCA on our image data… what now? Well, let’s see how well we can reconstruct the data using only the first 67 PCs, which contained 95% of the variance. To achieve this, we need to do a bit of matrix multiplication as well as add the mean back to un-center the data.\nThe PCA reconstructed data = the PC scores (matrix dimensions 191x191) x the transpose of the eigenvectors (matrix dimensions 191x187500) + mean\nIf we wanted to take only the top k PCs, which in our case I wanted the top 67, then we simply subset the PC scores and eigenvectors from 191 to 67. I then reconstruct the image matrix from the single image vector, normalize the values from 0-1, and save all the newly reconstructed image matrices in a single list.\n\n\nCode\n### reconstructing training data ###\n\n# use pca_results to reconstruct training data fully\nreconstructed_train_data_allpcs = pca_result$x %*% t(pca_result$rotation)\n# scale data back to center\nreconstructed_train_data_allpcs = scale(reconstructed_train_data_allpcs, center = -pca_result$center, scale = FALSE)\n\n# now use just 95% variation explained pcs\nreconstructed_train_data_95 = pca_result$x[,1:pcs_for_95] %*% t(pca_result$rotation[,1:pcs_for_95])\n# scale data back to center\nreconstructed_train_data_95 = scale(reconstructed_train_data_95, center = -pca_result$center, scale = FALSE)\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", length(length(train_indices)))\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:length(train_indices)) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(reconstructed_train_data_allpcs[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Function to normalize the matrix \nnormalize_matrix = function(matrix) {\n  min_value = min(matrix)\n  max_value = max(matrix)\n  normalized_matrix = (matrix - min_value) / (max_value - min_value)\n  return(normalized_matrix)\n}\n\n# Normalize each matrix in the list\nflags_reconstructed_train_data_allpcs = lapply(matrices_250x250x3, normalize_matrix)\n\n# now do for 95% variation\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", length(length(train_indices)))\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:length(train_indices)) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(reconstructed_train_data_95[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Normalize each matrix in the list\nflags_reconstructed_train_data_95 = lapply(matrices_250x250x3, normalize_matrix)\n\n\nNow let’s view how well PCA reconstructed the data! I utilized ggplot to plot three versions of five different flags: the original resized training image, the reconstructed resized training image with all PCs, the reconstructed resized training image with 67 PCs.\n\n\nCode\n# for loop getting 5 examples\nfor (i in 1:5) {\n  \n  # getting image - making cimg from imager package and then df\n  img1 = as.data.frame(as.cimg(imgs_final[[train_indices[i]]]), wide = \"c\")  |&gt; \n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"Original Image\")\n  \n  # getting image - making cimg from imager package and then df\n  img2 = as.data.frame(as.cimg(flags_reconstructed_train_data_allpcs[[i]]), wide = \"c\") |&gt;\n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"Fully Reconstructed Image\")\n\n  # getting image 1 - making cimg from imager package and then df\n  img3 = as.data.frame(as.cimg(flags_reconstructed_train_data_95[[i]]), wide = \"c\") |&gt;\n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"67 PCs Reconstructed Image\")\n  \n  # combining to all images\n  all_images = rbind(img1, img2, img3)\n  \n  # make the levels of img variable what i want\n  all_images$image_type = factor(all_images$image_type, levels = c(\"Original Image\", \"Fully Reconstructed Image\",\"67 PCs Reconstructed Image\"))\n\n  # plot image\n  print(ggplot(all_images,aes(y,x))+\n    # getting rgb\n    geom_raster(aes(fill=rgb_value))+\n    # fill image\n    scale_fill_identity() +\n    # reverse y axis\n    scale_y_reverse() + \n    # wrap by image\n    facet_wrap(.~image_type) +\n    # get rid of theme\n    theme_void() +\n    # bigger font size\n    theme(strip.text = element_text(size = 12)))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you think? Is 67 PCs enough to reconstruct the data? Since all 191 PCs were used to reconstruct the flags in the middle, we expect it to look identical to the original image as it preserves 100% of the variation. However, using only 67 PCs, or 95% of the variation of the flag images, we see more grainy images with artifacts from other flags. A lot of the reconstructed flags seems to have X’s on them along with a symbol from another flag within the dataset. If my goal was to compress these images by reducing the dimensions using PCA, I personally would use more PCs to do so. However, these reconstructed flags might be enough if one were trying to do classification or another machine learning task. So it really depends on what the goal of your project is to determine how many PCs to keep."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#pca-on-the-test-data",
    "href": "blog/2023-10-20-pca-on-images/index.html#pca-on-the-test-data",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "PCA on the test data",
    "text": "PCA on the test data\nNow let’s see how well the PCA on the training data performs on the testing data! To reconstruct the test data, we first use the “predict” function in R to obtain the PC scores for the training data using the PCA results from the training data. We can then use the same formula we used for training set to get the reconstructed data for the testing set. Finally, we can plot these flags using ggplot to visualize the reconstructed data.\n\n\nCode\n### Project testing data ###\n\n# first predict the testing data\ntest_data_projected = predict(pca_result, newdata = test_data)\n\n# Reconstruct the test_data from the projected data using all pcs\nreconstructed_test_data_allpcs = test_data_projected %*% t(pca_result$rotation)\n\n# reconstruct using 95%\nreconstructed_test_data_95 = test_data_projected[,1:pcs_for_95] %*% t(pca_result$rotation[,1:pcs_for_95])\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", length(length(test_indices)))\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:length(test_indices)) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(reconstructed_test_data_allpcs[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Normalize each matrix in the list\nflags_reconstructed_test_data_allpcs = lapply(matrices_250x250x3, normalize_matrix)\n\n\n# now with 95%\n\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", length(length(test_indices)))\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:length(test_indices)) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(reconstructed_test_data_95[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Normalize each matrix in the list\nflags_reconstructed_test_data_95 = lapply(matrices_250x250x3, normalize_matrix)\n\n# for loop getting 5 examples\nfor (i in 1:5) {\n  \n  # getting image - making cimg from imager package and then df\n  img1 = as.data.frame(as.cimg(imgs_final[[test_indices[i]]]), wide = \"c\")  |&gt; \n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"Original Image\")\n  \n  # getting image - making cimg from imager package and then df\n  img2 = as.data.frame(as.cimg(flags_reconstructed_test_data_allpcs[[i]]), wide = \"c\") |&gt;\n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"Fully Reconstructed Image\")\n\n  # getting image 1 - making cimg from imager package and then df\n  img3 = as.data.frame(as.cimg(flags_reconstructed_test_data_95[[i]]), wide = \"c\") |&gt;\n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"67 PCs Reconstructed Image\")\n  \n  # combining to all images\n  all_images = rbind(img1, img2, img3)\n  \n  # make the levels of img variable what i want\n  all_images$image_type = factor(all_images$image_type, levels = c(\"Original Image\", \"Fully Reconstructed Image\",\"67 PCs Reconstructed Image\"))\n\n  # plot image\n  print(ggplot(all_images,aes(y,x))+\n    # getting rgb\n    geom_raster(aes(fill=rgb_value))+\n    # fill image\n    scale_fill_identity() +\n    # reverse y axis\n    scale_y_reverse() + \n    # wrap by image\n    facet_wrap(.~image_type) +\n    # get rid of theme\n    theme_void() +\n    # bigger font size\n    theme(strip.text = element_text(size = 12)))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow you can observe that even when using all 191 PCs to reconstruct the test data, the result isn’t perfect. However, this makes sense! Since PCA was not performed on the testing data it likely contains new and unseen information for the algorithm. In other words, there exists variation within this new test dataset that was not present in the training dataset. Therefore, achieving a perfectly reconstructed image is not possible.\nI would argue that the difference between using 191 PCs or 67 PCs in the test case isn’t very significant. Particularly when comparing it to the notable differences in the training dataset, the reconstruction quality doesn’t appear to be too bad. There is one interesting observation we can make regarding the last image featuring the trident in the center. It’s clear that the reconstructed data was unable to accurately recreate this symbol. Instead, we see a depiction of a star in the middle image and a distinct-ish symbol in the far-right image. This suggests that there likely wasn’t a similar symbol present in the training dataset, causing PCA to struggle in reconstructing it."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#visualizing-the-principal-components",
    "href": "blog/2023-10-20-pca-on-images/index.html#visualizing-the-principal-components",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "Visualizing the principal components",
    "text": "Visualizing the principal components\nTo finish up, lets take a look at the top 10 PCs in image form. To do this, we simply look at the transpose of the rotation variable within pca_results. We transform this single vector into image form via the same process as before, and visualize using ggplot.\n\n\n\nCode\n### Plot PCs ###\n\n# get pcs\nprincipal_components = t(pca_result$rotation)\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", dim(principal_components)[1])\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:dim(principal_components)[1]) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(principal_components[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Normalize each matrix in the list\nflags_principal_components = lapply(matrices_250x250x3, normalize_matrix)\n\n# want to add all images to this dataframe for ggplot\nall_images = data.frame()\n\n# for loop getting 5 examples\nfor (i in 1:10) {\n  # getting image - making cimg from imager package and then df\n  img = as.data.frame(as.cimg(flags_principal_components[[i]]), wide = \"c\")  |&gt; \n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt; \n    # make class labels\n    mutate(img_num = paste(\"PC\",i))\n  \n  all_images = rbind(all_images, img)\n  \n  # make the levels of img variable what i want\n  all_images$img_num = factor(all_images$img_num, levels = c(\"PC 1\",\"PC 2\",\"PC 3\",\"PC 4\",\"PC 5\",\n                                                                   \"PC 6\",\"PC 7\",\"PC 8\",\"PC 9\",\"PC 10\"))\n}\n\n# plot image\nprint(ggplot(all_images,aes(y,x))+\n  # getting rgb\n  geom_raster(aes(fill=rgb_value))+\n  # fill image\n  scale_fill_identity() +\n  # facet_wrap\n  facet_wrap(.~ img_num, nrow = 2, ncol = 5) +\n  # reverse y axis\n  scale_y_reverse() + \n  # get rid of theme\n  theme_void() +\n  # bigger font size\n  theme(strip.text = element_text(size = 12)))\n\n\n\n\n\n\n\n\n\nPersonally, I think this is the coolest part! It’s amazing to visually observe the variations captured by each PC. One of the most compelling examples that caught my attention is the comparison between PC6 and PC9. Both PCs capture the flag symbol in the top left corner and exhibit similar colors. However, PC6 splits its colors horizontally, while PC9 splits them vertically. They complement each other in a nice way, showcasing the diverse ways information is reflected across the PCs! Some other PCs, such as PC3 and PC4, have a striking resemblance to specific flag images they were trained on. On the other hand, PCs like PC7 and PC10 seem to combine elements from two or more flags in the dataset, creating cool hybrid representations.\nThese results actually made me curious about what the appearance of the PCs beyond the top 10 would look like. So let’s plot ten of them side by side and compare! This will hopefully provide further insights into the patterns and variations captured by these additional PCs.\n\n\nCode\n### Plot PCs ###\n\n# want to add all images to this dataframe for ggplot\nall_images = data.frame()\n\n# for loop getting 5 examples\nfor (i in seq(11,191,20)) {\n  # getting image - making cimg from imager package and then df\n  img = as.data.frame(as.cimg(flags_principal_components[[i]]), wide = \"c\")  |&gt; \n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt; \n    # make class labels\n    mutate(img_num = paste(\"PC\",i))\n  \n  all_images = rbind(all_images, img)\n  \n  # make the levels of img variable what i want\n  all_images$img_num = factor(all_images$img_num, levels = sprintf(\"PC %d\", seq(11, 191, 20)))\n}\n\n# plot image\nprint(ggplot(all_images,aes(y,x))+\n  # getting rgb\n  geom_raster(aes(fill=rgb_value))+\n  # fill image\n  scale_fill_identity() +\n  # facet_wrap\n  facet_wrap(.~ img_num, nrow = 2, ncol = 5) +\n  # reverse y axis\n  scale_y_reverse() + \n  # get rid of theme\n  theme_void() +\n  # bigger font size\n  theme(strip.text = element_text(size = 12)))\n\n\n\n\n\n\n\n\n\nExcept for PC11, which is only one rank away from the top 10, these PCs exhibit distinct characteristics when compared to the top PCs. Rather than focusing on the overall color of the flag images, they appear to emphasize patterns within the dataset. Interestingly, the final PC essentially corresponds to a completely black image. Checking our variance_explained variable, we find it accounts for 7.620894e-35, which is approximately 0% of the dataset highlighting its lack of information. Additionally, many of the later PCs feature specific symbols like stars and shields, which may explain why our reconstructed test data failed to capture the trident but instead exhibited various other symbols such as a star. This serves as an important reminder that predicting new data can be challenging, particularly when it introduces novel variations or, in our case, flag symbols in images that were previously unseen by PCA. This concept holds true across most (if not all) machine learning applications, especially deep learning where the availability of comprehensive training datasets can be a major constraint."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#final-thoughts",
    "href": "blog/2023-10-20-pca-on-images/index.html#final-thoughts",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "Final thoughts",
    "text": "Final thoughts\nHonestly, this was a really fun project to do! It really helped me understand PCA better and how it can be utilized with images. I also was able to work on my R skills using matrix multiplication, imaging packages, and ggplot! I hope this project will help if you are struggling to understand how to do PCA on images in R. I did some searching and it was hard to find a good resource that went through step-by-step of the process… so hopefully this helped and my code is commented well enough for anyone to understand 😊 Let me know if you have any questions about the process or why I did what I did!\nThanks for reading 🥳🥳\nCaleb 🎲"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks/Posters",
    "section": "",
    "text": "2023\nPosters:\n\n“Phenotyping of Heterogenous Live Cell Motility and Morphology,” Dr. M. Judah Folkman Research Day, Boston Children’s Hospital & Harvard Medical School, Boston, MA, 2023.\n\n\n\n\n\n\n2022\nTalks:\n\n“Machine Learning Approaches Applied to the Prediction of Covid-19 Spread and Cell Motility Phenotyping,” Vascular Biology Program Work in Progress, Boston Children’s Hospital & Harvard Medical School, Boston, MA, 2022.\n\n\n\n\n\n\n“Deconvolution of Cellular Heterogeneity for Sub-Type Discovery by Analyzing Feature Variation,” Vascular Biology Program Work in Progress, Boston Children’s Hospital & Harvard Medical School, Boston, MA, 2022.\n\n\n\n\n\nPosters:\n\n“Deep-Hetero: A Deep Metric Learning with UMAP-based Clustering Approach for Identifying Heterogeneity in Cells,” Dr. M. Judah Folkman Research Day, Boston Children’s Hospital & Harvard Medical School, Boston, MA, 2022.\n\n\n\n2021\nPosters:\n\n“Ultrasound Microbubble Tumor Analysis,” Focused Ultrasound Foundation Summer Intern Presentations, Focused Ultrasound Foundation, Charlottesville, VA, 2021."
  }
]