[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software and data packages",
    "section": "",
    "text": "TBD\n\n\n\nTBD"
  },
  {
    "objectID": "software.html#bioconductor",
    "href": "software.html#bioconductor",
    "title": "Software and data packages",
    "section": "",
    "text": "TBD\n\n\n\nTBD"
  },
  {
    "objectID": "software.html#cran",
    "href": "software.html#cran",
    "title": "Software and data packages",
    "section": "CRAN",
    "text": "CRAN\n\nSoftware packages\nTBD"
  },
  {
    "objectID": "software.html#python",
    "href": "software.html#python",
    "title": "Software and data packages",
    "section": "Python",
    "text": "Python\n\nSoftware packages\nTBD"
  },
  {
    "objectID": "software.html#github",
    "href": "software.html#github",
    "title": "Software and data packages",
    "section": "GitHub",
    "text": "GitHub\n\nSoftware packages\nTBD\n\n\nData packages\nTBD"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Check out my google scholar or ORCID for a list of my publications!"
  },
  {
    "objectID": "projects.html#publications",
    "href": "projects.html#publications",
    "title": "Projects",
    "section": "",
    "text": "Check out my google scholar or ORCID for a list of my publications!"
  },
  {
    "objectID": "projects.html#new-projects",
    "href": "projects.html#new-projects",
    "title": "Projects",
    "section": "new projects ?",
    "text": "new projects ?\nI am working in Dr. Jean Fan’s lab doing Spatial Transcriptomics analysis, so I look forward to what the future has in store!"
  },
  {
    "objectID": "projects.html#evidence-of-off-target-probe-binding-in-the-10x-genomics-xenium-v1-human-breast-gene-expression-panel-compromises-accuracy-of-spatial-transcriptomic-profiling-2025",
    "href": "projects.html#evidence-of-off-target-probe-binding-in-the-10x-genomics-xenium-v1-human-breast-gene-expression-panel-compromises-accuracy-of-spatial-transcriptomic-profiling-2025",
    "title": "Projects",
    "section": "Evidence of off-target probe binding in the 10x Genomics Xenium v1 Human Breast Gene Expression Panel compromises accuracy of spatial transcriptomic profiling (2025)",
    "text": "Evidence of off-target probe binding in the 10x Genomics Xenium v1 Human Breast Gene Expression Panel compromises accuracy of spatial transcriptomic profiling (2025)\nAbstract\nThe accuracy of spatial gene expression profiles generated by probe-based in situ spatially-resolved transcriptomic technologies depends on the specificity with which probes bind to their intended target gene. Off-target binding, defined as a probe binding to something other than the target gene, can distort a gene’s true expression profile, making probe specificity essential for reliable transcriptomics. Here, we investigate off-target binding in the 10x Genomics Xenium v1 Human Breast Gene Expression Panel. We developed a software tool, Off-target Probe Tracker (OPT), to identify putative off-target binding via alignment of probe sequences and found at least 21 out of the 280 genes in the panel impacted by off-target binding to protein-coding genes. To substantiate our predictions, we leveraged a previously published Xenium breast cancer dataset generated using this gene panel and compared results to orthogonal spatial and single-cell transcriptomic profiles from Visium CytAssist and 3’ single-cell RNA-seq derived from the same tumor block. Our findings indicate that for some genes, the expression patterns detected by Xenium demonstrably reflect the aggregate expression of the target and predicted off-target genes based on Visium and single-cell RNA-seq rather than the target gene alone. Overall, this work enhances the biological interpretability of spatial transcriptomics data and improves reproducibility in spatial transcriptomics research.\nCheck out the preprint here."
  },
  {
    "objectID": "projects.html#subtype-discovery-method-utilizing-scrna-seq-and-microarray-data-2025",
    "href": "projects.html#subtype-discovery-method-utilizing-scrna-seq-and-microarray-data-2025",
    "title": "Projects",
    "section": "subtype discovery method utilizing scRNA-seq and microarray data (2025)",
    "text": "subtype discovery method utilizing scRNA-seq and microarray data (2025)\n\nOur method, PHet, is able to distinguish multiple subtypes of data given only two labels (control and case)\n\n\n\n\nSummary of Abstract\nIn disease diagnosis and targeted therapy, discovering subtypes is crucial as cells or patients can exhibit varied responses to treatments. Hence, understanding the heterogeneity of disease states is vital for comprehending pathological processes. However, selecting features for subtyping from high-dimensional datasets is challenging, with many algorithms focusing on known disease phenotypes and potentially overlooking valuable subtyping information. Our study aimed to address this issue by identifying feature sets that preserve heterogeneity while discriminating known disease states. Through a data-driven approach combining feature clustering and deep metric learning, we developed a statistical method called PHet (Preserving Heterogeneity). This method effectively identifies a minimal set of features that maintain heterogeneity while maximizing the quality of subtype clustering. PHet outperformed previous methods in identifying disease subtypes using microarray and single-cell RNA-seq datasets. Our research provides an innovative feature selection method that facilitates personalized medicine and enhances understanding of disease heterogeneity. I am co-first author with my former labmate Dr. Abdurrahman Abul-Basher.\nCheck out the publication in Nature Communications here."
  },
  {
    "objectID": "projects.html#image-analysis-of-live-cell-imaging-2022-2023",
    "href": "projects.html#image-analysis-of-live-cell-imaging-2022-2023",
    "title": "Projects",
    "section": "image analysis of live-cell imaging (2022-2023)",
    "text": "image analysis of live-cell imaging (2022-2023)\nI worked under Dr. Kwonmoo Lee at Boston Children’s Hospital as a Research Assistant for two years after undergrad. My work included using various Convolutional Neural Networks (CNNs) for cell segmentation, utilizing cell tracking algorithms, honing my image manipulation skills (Fiji), working on my research writing aptitude, and much more! It was an incredible experience that has led me to where I am now :) Below are two papers I had the privilege of working on:\n\nThe Lee Lab developed a deep learning-based pipeline termed MARS-Net . While I did not contribute to it’s development, I helped the first author write the protocol for running it here.\nI helped in optimizing the hyper parameters for the R-CNN in FNA-Net, a deep-learning based ensemble model aimed to screen the adequacy of unstained thyroid fine needle aspirations (FNA). Ideally, this will streamline the diagnostic process by eliminating the need for staining and expert interpretation.\nMy biggest project was a subtype discovery method termed PHet. The abstract and preprint are above!"
  },
  {
    "objectID": "projects.html#undergraduate-research-at-the-university-of-virginia-2018-2021",
    "href": "projects.html#undergraduate-research-at-the-university-of-virginia-2018-2021",
    "title": "Projects",
    "section": "undergraduate research at the University of Virginia (2018-2021)",
    "text": "undergraduate research at the University of Virginia (2018-2021)\nI started my research journey by collaborating with two incredible mentors, Dr. Tianxi Li and Dr. Frederic Padilla. Under Dr. Li’s guidance, I honed my coding skills in Python and R by converting his randnet package from R to Python. Later, I teamed up with Dr. Padilla to explore the effects of focused ultrasound on mouse tumors, which involved applying machine learning algorithms and statistical tests to understand its impact. In my final project my fourth year, I focused on classifying brain tumor regions in contrast-enhanced ultrasound images using microbubble intensity. I presented my final work via zoom (bc covid) where I expressed how well various unsupervised and supervised methods faired in this task. Though I didn’t publish any papers or achieve significant outcomes, this experience was incredibly valuable and marked the beginning of my research journey. I am immensely grateful to both mentors for the opportunity they gave me, as I don’t think I would be where I am now without them!"
  },
  {
    "objectID": "projects.html#github-contributions",
    "href": "projects.html#github-contributions",
    "title": "Projects",
    "section": "github contributions",
    "text": "github contributions"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Here is my updated CV as of 08/09/2024"
  },
  {
    "objectID": "blog/2023-11-05-fantasy-football-analysis/index.html",
    "href": "blog/2023-11-05-fantasy-football-analysis/index.html",
    "title": "An Analysis on my 2023 Fantasy Football League (So Far)",
    "section": "",
    "text": "My friends and I are always talking about fantasy football. You may read that and think I’m kidding, but if you look at my phone screen time since football season started the top two are 1. The ESPN Fantasy App and 2. The IPhone Messages App because I am messaging my friends about fantasy. So no, I am not kidding. It’s probably not worth it because I won’t win this year anyways, but eh - it’s fun and I enjoy it :)\n\n\n\n\n\nESPN Fantasy App Logo\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFantasy Football can and will likely become addicting if you play, so fair warning to those who have never played before and are interested 😅\n\n\n\n\n\nCode\n# Read in my package ahead of time\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(paletteer)\nlibrary(stringr)"
  },
  {
    "objectID": "blog/2023-11-05-fantasy-football-analysis/index.html#brief-introduction",
    "href": "blog/2023-11-05-fantasy-football-analysis/index.html#brief-introduction",
    "title": "An Analysis on my 2023 Fantasy Football League (So Far)",
    "section": "",
    "text": "My friends and I are always talking about fantasy football. You may read that and think I’m kidding, but if you look at my phone screen time since football season started the top two are 1. The ESPN Fantasy App and 2. The IPhone Messages App because I am messaging my friends about fantasy. So no, I am not kidding. It’s probably not worth it because I won’t win this year anyways, but eh - it’s fun and I enjoy it :)\n\n\n\n\n\nESPN Fantasy App Logo\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFantasy Football can and will likely become addicting if you play, so fair warning to those who have never played before and are interested 😅\n\n\n\n\n\nCode\n# Read in my package ahead of time\nlibrary(tidyverse)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(paletteer)\nlibrary(stringr)"
  },
  {
    "objectID": "blog/2023-11-05-fantasy-football-analysis/index.html#the-questions-and-data",
    "href": "blog/2023-11-05-fantasy-football-analysis/index.html#the-questions-and-data",
    "title": "An Analysis on my 2023 Fantasy Football League (So Far)",
    "section": "The Questions and Data",
    "text": "The Questions and Data\nWith this being said, I wanted to do a more in depth analysis of some fantasy football statistics so far this year. I did some research and found two awesome blog posts that helped get me started on reading in data and interesting graphs to make [1] [2]. The goal was to answer questions like:\n\nHow accurate are your weekly projections?\nWhat teams are under or over performing each week compared to others?\nWhere do most of the points come from for each team?\n\nMy hope is to add more to this analysis as time goes on, so let me know if you have a question you are interested in!\nI have two main audiences I want this to reach:\n\nMy fantasy football friends\nOthers who do fantasy football and might find my analysis interesting\n\nFor those of you who may not understand fantasy football, I won’t go into much depth on it so check out this short post by ESPN that explains it [3].\nSo for the data, I decided to use my football fantasy league of this year (2023). I could have done last year for a more complete analysis of how I didn’t win, but this give my league things to talk about now and I can continually update the analysis each week (which seemed pretty cool to me). It turns out I am not the first person that wanted this data, so lucky for me a guy named Tim Bryan made a function to extract all the data from the website [4]. Click here to go to the specific repository I used from him.\nHowever, not everything worked the first try and I had to make some changes to his code to load in all the data without some errors. Feel free to check my github repo for what I did as well as the data used. So, after some tweeks I was successfully able to read in and save everything I needed to a .csv file in my local directory which I then pushed to my github. Here is a data dictionary for the variables I was using with two different datasets:\n\n\nData Dictionary for Scoring Variables\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDescription\nExample\n\n\n\n\nWeek\ndbl\nWeek for each football games played\nThere are 17 Weeks total\n\n\nPlayerName\nchr\nName of football player\nJoe Burrow\n\n\nPlayerScoreActual\ndbl\nThe actual points a player scored in a given week\n21.2\n\n\nPlayerScoreProjected\ndbl\nThe projected points a player is to score in a given week\n19.0\n\n\nPlayerRosterSlot\nchr\nWhat position that player is playing in a given week\nWR\n\n\nTeamName\nchr\nThe team who has a given player\nTeam Caleb (me!)\n\n\n\n\nData Dictionary for Weekly Match-up Variables\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDescription\nExample\n\n\n\n\nWeek\ndbl\nWeek for each football games played\nThere are 17 Weeks total\n\n\nName1\nchr\nName of team 1\nTeam Caleb (me!)\n\n\nScore1\ndbl\nThe total points Name1 scored in a given week\n102.7\n\n\nName2\nchr\nName of team 2\nTeam BL\n\n\nScore2\ndbl\nThe total points Name2 scored in a given week\n121.3\n\n\nType\nfactor\nRegular season or Playoff game\nRegular\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere is also a brief dictionary for common abbreviations you will see throughout this blog and in fantasy football:\nK = Kicker\nD/ST = Defense and Special Teams\nTE = Tight End\nWR = Wide Receiver\nRB = Running Back\nQB = Quarterback\nFLEX = Can be any position listed above\n\n\nFinally, I de-identified all the data but mine to where my friends would know who they are but no one else can identify them. This data is coming from the ESPN Fantasy Website so thank you to them for allowing me to extract it [5]!\nSo now, let’s go ahead and read in the data I obtained using python (on my github). I do some pre-processing to remove weeks that haven’t been played yet and other columns I don’t need.\n\n\n\nCode\n# read in data\ndata = read_csv(\"data/league_data_2023_deidentified.csv\")\n\n# read in matchup data\nmatchups = read_csv(\"data/league_matchups_2023_deidentified.csv\")\n# get rid of some of the data as it duplicated it\nmatchups = matchups[1:65,]\n\n# NOTE: edit WEEK based on current week\ndata = data |&gt; \n  # get rid of weeks 9-17\n  filter(!Week %in% c(14:17)) |&gt; \n  # drop playerfantasyteam\n  select(-PlayerFantasyTeam)\n\nmatchups = matchups |&gt; \n  # get rid of weeks 9-17\n  filter(!Week %in% c(14:17))"
  },
  {
    "objectID": "blog/2023-11-05-fantasy-football-analysis/index.html#the-analysis",
    "href": "blog/2023-11-05-fantasy-football-analysis/index.html#the-analysis",
    "title": "An Analysis on my 2023 Fantasy Football League (So Far)",
    "section": "The Analysis",
    "text": "The Analysis\nThe first question I wanted to look at was one I was interested coming into this analysis. How accurate are your weekly projections? To me, it seems like they never are haha - I could be projected super high one week and then score 20 points below or vice versa any given week. So let’s look at the plot below:\n\n\nCode\n# plot showing weekly projections to actual score\n# data\ndata|&gt;\n  # groupby team name and week\n  group_by(TeamName, Week)|&gt;\n  # get rid of bench players as they don't count towards score\n  filter(PlayerRosterSlot != \"Bench\")|&gt;\n  # get the sum of your actual player roster and projected \n  summarize(weekly_score = sum(PlayerScoreActual),\n            projected_weekly_score = sum(PlayerScoreProjected))|&gt;\n  # begin plotting\n  ggplot(aes(x = Week, y = weekly_score, color = TeamName)) +\n  # add solid line for actual total\n  geom_line(aes(linetype = \"solid\", show.legend = FALSE)) +\n  # dashed line for projected total\n  geom_line(aes(y = projected_weekly_score, linetype = \"dashed\")) +\n  # add point to easily see week\n  geom_point(aes(y=weekly_score), size=1) +\n  # average projected\n  # geom_hline(aes(yintercept=mean(projected_weekly_score), linetype = \"dotted\", color=\"red\")) +\n  # facet wrap by teamname\n  facet_wrap(~ TeamName) +\n  # add labels\n  labs(\n    title = \"Weekly Actual and Projected Scores by Team\", \n    x = \"Week\", \n    y = \"Scores\",\n    caption = \"Data Source: ESPN Fantasy Website\",\n    subtitle = str_wrap(\"The plot reveals several interesting trends. Notably, Team BD and Team JN consistently maintain \n                        a steady average of projected points per week. Meanwhile, teams such as myself and Team BL have\n                        experienced a mix of both impressive and lackluster weeks. On the other hand, Team GM has had an\n                        incredible season so far, consistently delivering outstanding performances week after week.\n                        Lastly, a few teams like Team RW and Team TP have notably underperformed in multiple weeks.\",\n                        width = 87)) +\n  # add manual labels\n  scale_linetype_manual(values = c(\"dashed\", \"solid\"), labels = c(\"Projected\", \"Actual\")) +\n  # set y limit\n  ylim(50, 180) +\n  # set colors for teams\n  scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # get rid of color legend but use linetype legend\n  guides(color = \"none\", linetype = guide_legend(title = \"Scores\")) +\n  # use classic theme\n  theme_classic() +\n  # edit text and legend\n  theme(axis.text.x = element_text(size = 12),\n  axis.text.y = element_text(size = 12),\n  plot.title = element_text(face= \"bold\", hjust = 0.5),\n  legend.position = c(.98, -0.12), legend.justification = c(1, 0)) +\n  scale_x_continuous(breaks = c(2, 4, 6, 8,10,12))\n\n\n\n\n\n\n\n\n\nWell, I think I was right in thinking they aren’t ideal but honestly a couple teams seem to follow it fairly well. Team BD is pretty consistent across the board so far, as with Team JN. However, there are some teams with large outlier weeks like Team GM who has had some insane weeks or Team TP who has had some not so great weeks. Overall, it is pretty interesting to see the actual points scored that week compared to the projected!\nThe next graph is probably my favorite for various reasons and was suggested by one of my friends in this league. What we wanted to look at was how your opponents score that week compared to the weekly average score, or in other words how much your opponent “went off” or “did horribly” compared to others. I then decided to go one step further to plot your score for that week as well, and you can see some really interesting and funny trends in this plot below.\n\n\nCode\n# Points against vs weekly average vs. opponents score\n# making new dataframe\nweekly_data = data|&gt;\n  # group by teamname and week\n  group_by(TeamName, Week)|&gt;\n  # get rid of bench players\n  filter(PlayerRosterSlot != \"Bench\")|&gt;\n  # get projected and weekly score for each team\n  summarize(weekly_score = sum(PlayerScoreActual),\n            projected_weekly_score = sum(PlayerScoreProjected)) \n\n# get points against using an inner join between weekly data and matchups data\n# this is so I know who is playing who each week\npts_against = inner_join(weekly_data, matchups, by = c(\"Week\")) |&gt; \n  # had to only keep rows with the teamname of the week looked at\n  filter(TeamName == Name1 | TeamName == Name2)\n\n# add a column that is your opponent for that week\npts_against$opponent = ifelse(pts_against$TeamName == pts_against$Name1, pts_against$Name2,pts_against$Name1)\n# get the opponents score for that week\npts_against$opponent_score = ifelse(pts_against$TeamName == pts_against$Name1, pts_against$Score2,pts_against$Score1)\n\n# redfine dataframe\npts_against = pts_against |&gt; \n  # select only handful of columns now that are needed for plot\n  select(TeamName, Week, weekly_score, projected_weekly_score, opponent, opponent_score) |&gt; \n  # group by week\n  group_by(Week) |&gt; \n  # get mean weekly average to plot as well\n  mutate(weekly_average = mean(weekly_score))\n\n# init plot\nggplot(pts_against, aes(x = Week, color = TeamName)) +\n  # geom line for opponent score\n  geom_line(aes(y = opponent_score, linetype = \"dashed\"), color= \"gray\") +\n  # geom line for weekly average score\n  geom_line(aes(y = weekly_average, linetype = \"dotted\"), color= \"black\") + \n  # geom line for your score\n  geom_line(aes(y = weekly_score, linetype = \"solid\")) + \n  # wrap by team\n  facet_wrap(.~ TeamName) +\n  # set labs\n  labs(\n    title = \"Weekly Average Score Compared to You and Your Opponent's Score\", \n    x = \"Week\", \n    y = \"Scores\",\n    caption = \"Data Source: ESPN Fantasy Website\",\n    subtitle = str_wrap(\"There is a diverse range of teams, including consistent ones, teams with a lot of boom \n                        potential, and teams with a lot of bust potential. Notable outliers are Team GM, who \n                        frequently scores high, and Team TP, who frequently scores low. Generally, teams tend to \n                        score close to the average points per week, with a few teams deviating from this trend each\n                        week.\", width = 90)) +\n  # set custom labels for linetype\n  scale_linetype_manual(values = c(\"dashed\", \"dotted\", \"solid\"), labels = c(\"Opponents Score\", \"Weekly Average\", \"Your Score\")) +\n  # set colors for teamnames\n  scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # get only legend for linetype\n  guides(color = \"none\", linetype = guide_legend(title = \"Scores\")) +\n  # theme classe\n  theme_classic() +\n  # edit text and legend position\n  theme(axis.text.x = element_text(size = 12),\n  axis.text.y = element_text(size = 12),\n  plot.title = element_text(face= \"bold\", hjust = 0.5),\n  legend.position = c(.98, -.16), legend.justification = c(1, 0)) +\n  scale_x_continuous(breaks = c(2, 4, 6, 8, 10,12))\n\n\n\n\n\n\n\n\n\nWe see two teams, Team BD and JN, are both pretty “average” teams. However, what’s interesting about that is they are currently at the top of the league in wins (Through Week 9) therefore implying consistency may be the best in winning fantasy football. Team RW has fairly low scoring weeks, besides their one breakout week 5 against Team TP. Interestingly, Team TP started off strong but had a tough middle of the season and unlucky past two weeks getting just barely beat by their opponent.\nA lot of these trends can be explained more by BYE weeks (weeks that a NFL team has off), injuries, or injuries that affect other players (for example, QB Matt Stafford was out Week 9 and so WR Puka Nacua didn’t even score 5 points when he was projected 12). I think that is certainly the case for Team TP who has had a tough go at injuries this season. I personally have had a decent amount, like the number one overall pick Justin Jefferson being on IR and missing the past 4 weeks (note the drop in my score four weeks ago) 🙃.\nNext, I wanted to take a look at what position performs the best for each team and where the majority of their points are coming from. Take a look below!\n\n\nCode\n# Cumulative points over time and show distribution of each place\n\n# Define the desired order of the positions\nposition_order &lt;- c(\"K\", \"D/ST\",\"TE\", \"FLEX\",\"WR\",\"RB\", \"QB\")\n\n# get data\ndata |&gt; \n  # group by teamname and week\n  group_by(TeamName, PlayerRosterSlot) |&gt; \n  # filter IR players out\n  filter(PlayerRosterSlot != \"IR\") |&gt;\n  # gilter bench players out\n  filter(PlayerRosterSlot != \"Bench\") |&gt;\n  # get sum of weekly score\n  summarise(total_score_per_position = sum(PlayerScoreActual))  |&gt; \n  # factor rosterslot\n  mutate(PlayerRosterSlot = factor(PlayerRosterSlot, levels = position_order)) |&gt; \n  # plot in ggplot\n  ggplot(aes(x = TeamName, y = total_score_per_position, fill = PlayerRosterSlot)) +    \n  # geom bar of all positions\n  geom_bar(position=\"dodge\", stat=\"identity\") +\n    # change colors and add percent to y axis\n    # scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # labels\n  labs(\n      x = \"Team Name\",\n      y = \"Total Scores (Through Week 13)\",\n      fill = \"Positions\",\n      title = \"Total Score of Each Position For All Teams\",\n      caption = \"Data Source: ESPN Fantasy Website\",\n      subtitle = str_wrap(\"Wide receivers (WR) and running backs (RB) consistently dominate in point scoring for each\n                          team, with the quarterback (QB) following closely behind. The FLEX position seems variable\n                          among teams which makes sense as it allows any positional player to be utilized, which is why\n                          most people have either an RB or WR in that spot. Notably, positions such as kicker (K),\n                          defense/special teams (D/ST), and tight end (TE) exhibit relatively consistent performance\n                          across all teams.\", width = 87)) +\n  # change theme to linedraw\n  theme_linedraw() + \n  # edit text\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), \n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5)) +\n  # add colors to each position\n  scale_fill_manual(values = paletteer_d(\"NatParksPalettes::GrandCanyon\", 7))\n\n\n\n\n\n\n\n\n\nUnsurprisingly, RBs and WRs are the most valuable position as they score more than any other position. However, it is worth noting that there are two RB/WR slots in your lineup and only one of every other position. So, if we double a lot of team’s QB points one could argue they are the most valuable and score the most points. To me, it seems like positions other than RB, WR, or QB are all relatively similar throughout teams, with only a small variation. If you begin to add more context to this plot, like the fact that Team WN had Travis Kelce (who is easily the best TE and scores a lot more points than other TEs) for the past 8 weeks you see he has a much higher TE total. This is certainly an advantage, however Team WN did use their first round pick on Travis (yes, this is the guy currently dating Taylor Swift).\n\n\nI usually never draft a QB high, with my theory being in a 10 man league there are typically enough “average” QBs to be fine. This year, I decided to draft Joe Burrow much earlier than I usually do and it has come back to bite me in the butt because he has been pitiful. So, lesson learned and I will not be drafting QBs early anymore.\n\n\n\nJoe Burrow\n\n\nI added this plot later in the analysis because I found it pretty interesting. Take a look below!\n\n\nCode\n# boxplot of scores for my team\n\ndata |&gt; \n  # remove players\n  filter(!PlayerRosterSlot %in% c(\"Bench\", \"IR\")) |&gt; \n  # groupby team\n  group_by(TeamName) |&gt; \n  # init plot\n  ggplot(aes(x = TeamName, y = PlayerScoreActual, fill = TeamName)) +\n  # geom violin\n  geom_violin() +\n  geom_boxplot(width=.1, color=\"white\",outlier.shape = NA) +\n   # add colors\n  scale_fill_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # change to linedraw\n  theme_linedraw() + \n  # add theme\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), \n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 11)) +\n  # add labels\n  labs(\n    x = \"Team Name\",\n    y = \"Player's Scores (All Weeks)\",\n    title = \"Distribution of Player Scores for Each Team (Through Week 9)\",\n    fill = \"Team Name\",\n    caption = \"Data Source: ESPN Fantasy Website\",\n    subtitle = str_wrap(\"The violin plots display the distribution of all player scores for each team. The boxplots \n    within expresses the median, interquartile range, and estimated min/max based on the interquartile range. \n    Notably, Team DH and Team RW have had the best weeks by a player so far this season, while Team WR has had the \n    worst scores. Most teams exhibit a similar median score, however, Team GM stands out with a noticeably higher\n    median comparatively.\", width = 90)) +\n  # get rid of legend\n  guides(fill = \"none\", color = \"none\")\n\n\n\n\n\n\n\n\n\nWe see Team DH and Team RW have had some of the best weeks by a single player so far this season. Since we have such a tight leader board so far, meaning everyone has a fairly similar record, it makes sense that the median of each team is fairly similar. Also, we see some teams like Team WN has less variation within their player scores compared to Team GM and others with a large variation.I’m looking forward to seeing this distribution at the end of the season! Also I should note, this plot is excluding bench players.\nFinally, this last graph was made solely for my friends and I so that we can laugh at the thought of who is getting wrecked this year in fantasy. Take a look below for more details 😄\n\n\n\n\n\n\nWarning\n\n\n\nTeam TP, you might want to look away for for this one…\n\n\n\n\nCode\n# graph for worst loss differential\n\n# grab difference of scores\nmatchups_diff = matchups |&gt; \n  # find difference of scores\n    mutate(Difference = Score1 - Score2)\n\n# get team with lower score\nmatchups_diff$LowerScoreName = ifelse(matchups_diff$Difference &lt; 0, matchups_diff$Name1, matchups_diff$Name2)\n\n# get abs of difference and plot\nmatchups_diff |&gt; \n  # absolute the difference \n  mutate(Difference = abs(Difference)) |&gt; \n  # group by team\n  group_by(LowerScoreName) |&gt; \n  # get sum of differences for each team\n  summarise(sum_diff = sum(Difference)) |&gt; \n  # init plot\n  ggplot(aes(x = LowerScoreName, y = sum_diff, fill = LowerScoreName)) +\n  # geom bar for each team\n  geom_col(position = \"dodge\") +\n  # add colors\n  scale_fill_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # change to linedraw\n  theme_linedraw() + \n  # add theme\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), \n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5)) +\n  # add labels\n  labs(\n    x = \"Team Name\",\n    y = \"Total Loss Differential\",\n    title = \"Total Loss Differential For Each Team (Through Week 13)\",\n    fill = \"Team Name\",\n    caption = \"Data Source: ESPN Fantasy Website\",\n    subtitle = str_wrap(\"It appears that there are four distinct tiers of total loss differential observed thus far in the\n                         season. Noteworthy outliers include Team BD, whose minimal loss differential suggests consistent\n                        performance despite losing games, and Team TP, who consistently experiences significant deficits\n                        in each loss they have.\", width = 85)) +\n  # get rid of legend\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\nThis plot is expressing the worst Loss Differential… so sorry Team TP 😅 It’s not looking good for you! One thing to note is this is not counting how much you win, it is solely taking the weeks you lose and finding the differential between you and your opponent’s team score."
  },
  {
    "objectID": "blog/2023-11-05-fantasy-football-analysis/index.html#final-thoughts",
    "href": "blog/2023-11-05-fantasy-football-analysis/index.html#final-thoughts",
    "title": "An Analysis on my 2023 Fantasy Football League (So Far)",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nUltimately, I had a really fun time doing this analysis and writing this blog! Looking forward to updating the results as the weeks go on, and to see who will be my league’s crowned chamption this year 🥳\nTo summarize a bit of what my analysis consisted of - I analyzed data from my 2023 Fantasy Football League. I found interesting trends in the weekly match-ups, such as typically when a team over-performs they will get the W (but not always) or some teams went through a rough stretch of weeks. I also showed how RBs and WRs are typically the positions that score the most points, with other positions being more team dependent (aka you have the one good TE in the league). Our league is pretty tight this year in terms of rankings, so it was hard to see that a certain trend in the data leads you to be the best team. However, I think as the weeks go on that will certainly become more clear. My goal is to do more of a statistical analysis when the season is complete in order to step up my game for next year’s fantasy football season! 😄"
  },
  {
    "objectID": "blog/2023-11-05-fantasy-football-analysis/index.html#functions-used",
    "href": "blog/2023-11-05-fantasy-football-analysis/index.html#functions-used",
    "title": "An Analysis on my 2023 Fantasy Football League (So Far)",
    "section": "Functions Used",
    "text": "Functions Used\n\ndplyr/tidyr:\n\nfilter: filter rows based on conditions\nselect: select specific columns\nsummarise: calculate summary statistics\nmutate: create or modify a column\ngroup_by: group data based on variables\ninner_join: merge two datasets on key values\n\nggplot2:\n\ngeom_line: line plot\ngeom_point: points on line plot\ngeom_bar: bar plot\ngeom_col: column plot\ngeom_violin: violin plot\ngeom_boxplot: boxplot within violin plot"
  },
  {
    "objectID": "blog/2023-11-05-fantasy-football-analysis/index.html#references",
    "href": "blog/2023-11-05-fantasy-football-analysis/index.html#references",
    "title": "An Analysis on my 2023 Fantasy Football League (So Far)",
    "section": "References",
    "text": "References\n\n\n[1] J. Mannelly, “Analyzing your fantasy football season with python.” https://jman4190.medium.com/analyzing-your-fantasy-football-season-with-python-8c228262eae9.\n\n\n[2] S. Stimson, “Checking ESPN fantasy football projections with python.” https://stmorse.github.io/journal/espn-fantasy-projections.html.\n\n\n[3] ESPN, “Fantasy football for beginners: How to play fantasy football 2022.” https://www.espn.com/fantasy/football/story/_/id/34389554/fantasy-football-beginners-how-play-fantasy-football-2022, 2022.\n\n\n[4] “ESPN fantasy football github.” https://github.com/tbryan2/espnfantasyfootball.\n\n\n[5] “ESPN fantasy football.” https://www.espn.com/fantasy/football/."
  },
  {
    "objectID": "blog/might-try-blogging/index.html",
    "href": "blog/might-try-blogging/index.html",
    "title": "Might try blogging…",
    "section": "",
    "text": "I am thinking about blogging some things… not really specific topics or thoughts, just notes I want to jot down because why not."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "My wife, Celia, and I in Iceland!\n\n\n\nHi! My name is Caleb Hallinan and I am a second-year PhD candidate in BME at Johns Hopkins. During my graduate studies, I hope to develop user-friendly computational software specifically tailored for biologists who may not be as tech-savvy. I was recently introduced to spatial transcriptomics (ST) and immediately captivated, so I was thrilled to explore this field during a rotation with Dr. Jean Fan and now continue within the JEFworks Lab as a PhD Student! My proficiencies lie in the domains of data analysis, machine learning, and the development and implementation of high-end computational research tools.\nUltimately, I would love to become a Teaching Professor at a University one day. I have a passion for teaching and mentoring others, and what better way to do that than helping train the next generation of scientists :)\nWhen I’m not busy with research, you’ll find me playing/watching sports, hanging out with friends, or watching movies!"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "",
    "text": "Code\n### install and read in packages\n\n# install.packages(\"wordcloud\") # install this for wordcloud\n# install.packages(\"janitor\") # install for sentiment analysis\n# install.packages(\"devtools\")\n# devtools::install_github(\"ropensci/gutenbergr\")\n# install.packages(\"tm\")\n\nlibrary(tidyverse) # for dataframes\nlibrary(devtools) # for classic functions\nlibrary(gutenbergr) # for data\nlibrary(here) # for use of others if downloaded\nlibrary(tidytext) # for sentiment analysis\nlibrary(paletteer) # for colors for plots\nlibrary(tm) # topic model package\nlibrary(topicmodels) # topic model package"
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html#choosing-an-author",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html#choosing-an-author",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "Choosing an Author",
    "text": "Choosing an Author\nInitially, the researcher expressed interest in exploring Charles Darwin’s works due to their background in Biology. However, after conducting a sentiment analysis on his works it became evident that they resembled scientific papers more so than books. As a result, the analysis on Darwin yielded less informative and exciting results than anticipated. Consequently, the focus shifted to the works of Alexandre Dumas, a celebrated fiction and fantasy novelist renowned for works such as The Three Musketeers and The Count of Monte Cristo - a classic the researcher has personally read and thoroughly enjoyed. From the extensive collection the gutenbergr package has to offer of Dumas’ books, six books were selected: The Three Musketeers, Ten Years Later, Twenty Years After, The Black Tulip, The Count of Monte Cristo, Illustrated, and the The Wolf-Leader. According to Alexandre Dumas’ Wikipedia page, his books are classified into various types of fiction. The first three books belong to The D’Artagnan Romances trilogy, while the The Count of Monte Cristo, Illustrated and The Black Tulip fall into the adventure genre. The Wolf-Leader was the final novel analyzed and was categorized as one of Dumas’ fantasy books. With this background knowledge, by performing a sentiment analysis we can not only uncover differences and similarities between individual books but also between different genres within Dumas’ literary repertoire.\n\n\nCode\n### Getting Dumas, Alexandre Data ###\n\n\n# if file doesn't exist, download the data\nif (!file.exists(here(\"dumas.RDS\"))) {\n  \n  # message it wasn't found\n  message(\"File not found, downloading now...\")\n  \n  dumas = gutenberg_works() |&gt;\n  # group by author\n  group_by(author) |&gt;\n  # filter to get dumas\n  filter(author == \"Dumas, Alexandre\") |&gt;\n  # download data\n  gutenberg_download(meta_fields = \"title\", strip=TRUE)\n  \n  # save the files to RDS objects\n  saveRDS(dumas, file = here(\"dumas.RDS\"))\n  \n  # message when done\n  message(\"Finished!\") \n}\n\n\n# read in dumas\ndumas = readRDS(here(\"dumas.RDS\"))\n# use git_ignore to not push\n# usethis::use_git_ignore(\"dumas.RDS\")\n\n\n# get row numbers for dumas\ndumas = dumas |&gt; \n  # get rid of id\n  select(-gutenberg_id) |&gt;\n  # get rid of lines with no text \n  filter(text != \"\") |&gt; \n  # group by title\n  group_by(title) |&gt;\n  # make new column\n  mutate(linenumber = row_number()) |&gt; \n  ungroup()"
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html#sentiment-analysis",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html#sentiment-analysis",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nAfter conducting sentiment analysis on five of Alexandre Dumas’ well-known books, some interesting trends regarding his overall writing style and book-specific style emerged. Dumas’ writing exhibited a predominantly negative tone throughout, as evident by the cumulative sentiment declining constantly across all books. While this is only a small sample of his literary works, this trend is believed to be true based on our existing knowledge about Dumas.\n\n\nCode\n### Sentiment Analysis of Dumas, Alexandre Data ###\n\n# sample data here to save computational effort for rest of analysis\ndumas = dumas |&gt; \n  filter(title %in% c(\"The Three Musketeers\", \"Ten Years Later\", \"Twenty Years After\", \n                      \"The Black Tulip\", \"The Count of Monte Cristo, Illustrated\", \n                      \"The Wolf-Leader\"))\n\n\n# tokenize author\ntidy_dumas = dumas |&gt;\n  unnest_tokens(word, text)\n\n\n# # check to see what the top words are\n# tidy_dumas |&gt;\n#   # group by word\n#   group_by(word) |&gt;\n#   # count\n#   tally() |&gt; \n#   # arrange them\n#   arrange(desc(n)) \n# # NOTE: lots of the's and a's and such\n\n\n# # make a word cloud for just count of monte cristo\n# tt_dumas = tidy_dumas |&gt;\n#   # get count of monte\n#   filter(title == \"The Count of Monte Cristo, Illustrated\") |&gt;\n#   # get actual count of works\n#   count(word) |&gt;\n#   # arrange\n#   arrange(desc(n)) |&gt;\n#   # only get 200\n#   slice(1:200L)\n# # make a wordcloud of it\n# wordcloud::wordcloud(tt_dumas$word, tt_dumas$n)\n# # NOTE: lots of the's and a's and such\n\n\n# # can see words by by books\n# tidy_dumas |&gt;\n#   # count\n#   count(title, word) |&gt; \n#   #arrange\n#   arrange(desc(n)) |&gt; \n#   # group by title now\n#   group_by(title) |&gt; \n#   # get a couple\n#   slice(1L) \n\n\n# filter author with stop words\ntidy_dumas = tidy_dumas |&gt; \n  # get rid of stop words\n  anti_join(stop_words, by = \"word\")\n\n\n# # check with filtered author now\n# tidy_dumas |&gt;\n#   count(word) |&gt;\n#   arrange(desc(n))\n# # NOTE: lots of de, madame, replied, etc. that I should prob get rid of\n\n\n# top words by book\ntop_dumas_words = tidy_dumas |&gt;\n  # count with word and group by title\n  count(word, title) |&gt;\n  # arrange\n  arrange(desc(n)) |&gt; \n  # group by title\n  group_by(title) \n# top_dumas_words |&gt; slice(1:2)\n# NOTE: lots of names I should get rid of\n\n\n# # word cloud with no stop words\n# tt_dumas = tidy_dumas |&gt;\n#   # get count of monte again\n#   filter(title == \"The Count of Monte Cristo, Illustrated\") |&gt; \n#   # count\n#   count(word) |&gt;\n#   # arrange\n#   arrange(desc(n)) |&gt; \n#   # get 200\n#   slice(1:200L) \n# # make wordcloud\n# wordcloud::wordcloud(tt_dumas$word, tt_dumas$n)\n# # NOTE: \"count\" is highest word unsurpisingly\n\n\n# get bing sentiments\nbing = tidytext::sentiments \n# getting dupe words from janitor package\ndupes = bing |&gt; \n  janitor::get_dupes(word) \n# get rid of dupes\nbing = bing |&gt; \n  anti_join(dupes |&gt; filter(sentiment == \"positive\"))\n# check\n# anyDuplicated(bing$word) == 0\n# NOTE: good here!\n\n\n# top word sentiments with all words\n# top_dumas_words |&gt;\n#   slice(1:2) |&gt;\n#   left_join(bing, by = join_by(word))\n\n\n# # top word sentiments with only words with sentiment\n# top_dumas_words |&gt;\n#   # get rid of drop words\n#   filter(!word %in% dropwords) |&gt; \n#   inner_join(bing, by = join_by(word)) |&gt;\n#   slice(20:30) \n#   # NOTE: checked slices 1:30\n\n\n# Using this method to look at the text to determine if I should remove the word or not\n# or us a regex method\n# dumas |&gt;\n#   filter(str_detect(text, \"ah\"))\n\n# NOTES:\n# majesty: is usually \"his majesty\" or \"your majesty\" so remove all\n# honor: thought it would be like \"your honor\" but not really so keep it\n# prisoner: almost always its \"the prisoner\" so remove\n# master: mostly his master and master (referring to person), so add\n# excellency: same as your honor vibe, remove\n# stranger: not really used as person as much as I would have thought, so keep\n# de: found this when doing comparison to other others, just a name between names\n\n\n# going to get rid of some words here\ndropwords = c(\"majesty\", \"prisoner\", \"master\", \"excellency\", \"de\")\n\n\n# author sentiment\ndumassentiment = tidy_dumas |&gt; \n  # get rid of drop words\n  filter(!word %in% dropwords) |&gt; \n  # join with bing to get sentiment\n  inner_join(bing, by = join_by(word)) |&gt; \n  # TODO: what is this 80 for?\n  count(title, page = linenumber %/% 80, sentiment) |&gt; \n  # pivot wider data here\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n  # get sentiment score\n  mutate(sentiment = positive - negative) \n# check head of data\n# head(dumassentiment)\n# NOTE: looks good!\n\n\n# begin graphing\n\n\n# define the desired order of the legend\ndesired_order &lt;- c(\"The Three Musketeers\", \"Ten Years Later\", \"Twenty Years After\", \"The Black Tulip\", \n                   \"The Count of Monte Cristo, Illustrated\", \"The Wolf-Leader\")\n# define the desired colors for each title\ndesired_colors &lt;- c(\"darkblue\", \"blue\", \"lightblue\", \"darkred\", \"red\",\"darkgreen\")\n\n\n# change order for graph\ndumassentiment$title = factor(dumassentiment$title, levels = desired_order)\n\n\n\n\n# # ggplot of basic analysis of sentiment overtime\n# ggplot(dumassentiment, aes(page, sentiment, fill = title)) + \n#   # make bar graph, remove legend cause dont need\n#   geom_bar(stat = \"identity\", show.legend = FALSE) + \n#   # plot by book/title\n#   facet_wrap(~title, ncol = 3, scales = \"free_x\") +\n#   labs(\n#     x = \"Page Number\",\n#     y = \"Sentiment Score\",\n#     title = \"Sentiment Score Throughout Each Book\",\n#     caption = \"Data Source: Project Gutenberg\"\n#   ) + \n#   theme_bw() +\n#   theme(\n#     text = element_text(size = 11.5),\n#     plot.title = element_text(face = \"bold\", hjust = 0.5),\n#     legend.text = element_text(face = \"italic\")\n#     ) + \n#     scale_fill_manual(\n#     values = setNames(desired_colors, desired_order),\n#     breaks = desired_order\n#   )\n\n\n# plot the data\ng = dumassentiment|&gt; \n  # group by title\n  group_by(title) |&gt; \n  # get cumulative sentiment over time\n  mutate(sentiment = cumsum(sentiment), page = page/max(page)) |&gt; \n  # plot sentiment over time\n  ggplot(aes(page, sentiment, colour = title)) + \n  # make the line width bigger\n  geom_line(linewidth = 1.25) + \n  # labels\n  labs(\n    x = \"Percent of Total Pages (%)\",\n    y = \"Cumulative Sentiment\",\n    title = \"Trajectory of Sentiment Throughout Dumas' Works\",\n    caption = \"Data Source: Project Gutenberg\"\n  )\n\n# making transparent legend\ntransparent_legend = theme(legend.background = element_rect(fill = \"transparent\"), legend.key = \n                             element_rect(fill = \"transparent\", color = \"transparent\"))\n\n# plot\ng + \n  # add transparent legend\n  transparent_legend + \n  # change colors\n  # scale_color_brewer(type = \"qual\") + \n  # scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12), breaks = desired_order) +\n  # make specific colors go to specific titles\n  scale_colour_manual(\n    values = setNames(desired_colors, desired_order),\n    breaks = desired_order\n  ) +\n  # change x axis to percent\n  scale_x_continuous(labels = scales::percent_format()) + \n  # change theme to classic\n  theme_classic() + \n  # edit text\n  theme(\n    legend.position = c(0.25, 0.3), \n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    legend.text = element_text(face = \"italic\")\n    ) + \n  # change legend postion\n  guides(colour = guide_legend(title = \"Book\", override.aes = list(linewidth = 2)))\n\n\n\n\n\n\n\n\n\nWithin Dumas’ D’Artagnan Romances trilogy, represented by three shades of blue, The Three Musketeers had the lowest cumulative sentiment by the end of the book. The second book, Ten Years Later, contained a relatively more positive sentiment but remained negative overall. The final installment, Twenty Years After, displayed a negative cumulative sentiment similar to the first book. Hence, it appears that Dumas took readers on an emotional roller coaster from book to book while maintaining a generally negative connotation.\nAs for Dumas’ adventure genre books, The Count of Monte Cristo, Illustrated and The Black Tulip which are represented by shades of red, demonstrated significantly different sentiments over time. The Count of Monte Cristo, Illustrated had the lowest cumulative sentiment among all the books, while The Black Tulip approached a neutral sentiment by the end. Notably, The Count of Monte Cristo, Illustrated experienced a substantial drop in negative sentiment halfway through the book and continued its steep downward trend. Although the ending was not particularly positive, the researcher found the late and steep decline in negativity surprising given the “positive” elements (no spoilers) that are conveyed during that period if the book.\nLastly, a cumulative sentiment was performed on The Wolf-Leader, one of Dumas’ fantasy books. Despite its themes of werewolves, greed, power, and lust, the book had a slightly negative cumulative sentiment with minimal variation over time. Overall, it was somewhat surprising that the overall negative sentiment in this novel was not more pronounced.\nIt is important to note that certain words such as “majesty,” “prisoner,” “master,” and “excellency” were omitted from the analysis. These words were typically used as titles or names (e.g., “the prisoner” or “your excellency”) and did not significantly contribute to the books’ content. However, the exclusion of these words and the comparison between analyses with and without them did not lead to a significant deviation in the overall sentiment trend presented."
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html#topic-model-analysis",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html#topic-model-analysis",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "Topic Model Analysis",
    "text": "Topic Model Analysis\nNext, a topic model analysis using Latent Dirichlet allocation (LDA) was conducted. The Dumas’ dataset was combined with two new authors, Aristotle and Scott F. Fitzgerald, each contributing five distinct works. Aristotle’s selected books were The Poetics of Aristotle, The Categories, Politics: A Treatise on Government, Aristotle on the art of poetry, The Athenian Constitution. Scott F. Fitzgerald’s chosen works included This Side of Paradise, Flappers and Philosophers, The Beautiful and Damned, The Great Gatsby, All the Sad Young Men.\n\n\n\nCode\n### Getting Aristotle and Fitgerald Data ###\n\n\n# if file doesn't exist, download the data\nif (!file.exists(here(\"aristotle.RDS\"))) {\n  \n  # message it wasn't found\n  message(\"File not found, downloading now...\")\n  \n  aristotle = gutenberg_works() |&gt;\n  # group by author\n  group_by(author) |&gt;\n  # filter to get aristotle\n  filter(author == \"Aristotle\") |&gt;\n  # download data\n  gutenberg_download(meta_fields = \"title\", strip=TRUE)\n  \n  # save the files to RDS objects\n  saveRDS(aristotle, file = here(\"aristotle.RDS\"))\n  \n  # message when done\n  message(\"Finished!\") \n}\n\n\n# read in aristotle\naristotle = readRDS(here(\"aristotle.RDS\"))\n# use git_ignore to not push\n# usethis::use_git_ignore(\"aristotle.RDS\")\n\n\n# get row numbers for dumas\naristotle = aristotle |&gt; \n  # get rid of id\n  select(-gutenberg_id) |&gt;\n  # get rid of lines with no text \n  filter(text != \"\") |&gt; \n  # group by title\n  group_by(title) |&gt;\n  # make new column\n  mutate(linenumber = row_number()) |&gt; \n  ungroup()\n\n\n################################################\n\n\n# if file doesn't exist, download the data\nif (!file.exists(here(\"fitzgerald.RDS\"))) {\n  \n  # message it wasn't found\n  message(\"File not found, downloading now...\")\n  \n  fitzgerald = gutenberg_works() |&gt;\n  # group by author\n  group_by(author) |&gt;\n  # filter to get author\n  filter(author == \"Fitzgerald, F. Scott (Francis Scott)\") |&gt;\n  # download data\n  gutenberg_download(meta_fields = \"title\", strip=TRUE)\n  \n  # save the files to RDS objects\n  saveRDS(fitzgerald, file = here(\"fitzgerald.RDS\"))\n  \n  # message when done\n  message(\"Finished!\") \n}\n\n\n# read in aristotle\nfitzgerald = readRDS(here(\"fitzgerald.RDS\"))\n# use git_ignore to not push\nusethis::use_git_ignore(\"fitzgerald.RDS\")\n\n\n# get row numbers for dumas\nfitzgerald = fitzgerald |&gt; \n  # get rid of id\n  select(-gutenberg_id) |&gt;\n  # get rid of lines with no text \n  filter(text != \"\") |&gt; \n  # group by title\n  group_by(title) |&gt;\n  # make new column\n  mutate(linenumber = row_number()) |&gt; \n  ungroup()\n\n\n### Clean aristotle and fitzgerald Data, also do sentiment analysis but don't print ###\n\n\n# tokenize author\ntidy_aristotle = aristotle |&gt;\n  unnest_tokens(word, text)\n\n\n# filter author with stop words\ntidy_aristotle = tidy_aristotle |&gt; \n  # get rid of stop words\n  anti_join(stop_words, by = \"word\")\n\n\n# top words by book\ntop_aristotle_words = tidy_aristotle |&gt;\n  # count with word and group by title\n  count(word, title) |&gt;\n  # arrange\n  arrange(desc(n)) |&gt; \n  # group by title\n  group_by(title) \n# top_dumas_words |&gt; slice(1:2)\n\n\n# get bing sentiments\nbing = tidytext::sentiments \n# getting dupe words from janitor package\ndupes = bing |&gt; \n  janitor::get_dupes(word) \n# get rid of dupes\nbing = bing |&gt; \n  anti_join(dupes |&gt; filter(sentiment == \"positive\"))\n# check\n# anyDuplicated(bing$word) == 0\n# NOTE: good here!\n\n\n# # # top word sentiments with only words with sentiment\n# top_aristotle_words |&gt;\n#   # get rid of drop words\n#   filter(!word %in% dropwords) |&gt;\n#   inner_join(bing, by = join_by(word)) |&gt;\n#   slice(1:30)\n#   # NOTE: checked slices 1:30\n\n\n# Using this method to look at the text to determine if I should remove the word or not\n# or us a regex method\n# aristotle |&gt;\n#   filter(str_detect(text, \"cried\"))\n\n# NOTES:\n# majesty: is usually \"his majesty\" or \"your majesty\" so remove all\n# honor: thought it would be like \"your honor\" but not really so keep it\n# prisoner: almost always its \"the prisoner\" so remove\n# master: mostly his master and master (referring to person), so add\n# excellency: same as your honor vibe, remove\n# stranger: not really used as person as much as I would have thought, so keep\n\n\n# going to get rid of some words here\n# dropwords = c(\"majesty\", \"prisoner\", \"master\", \"excellency\")\n\n\n# # author sentiment\n# aristotlesentiment = tidy_aristotle |&gt; \n#   # get rid of drop words\n#   # filter(!word %in% dropwords) |&gt; \n#   # join with bing to get sentiment\n#   inner_join(bing, by = join_by(word)) |&gt; \n#   # TODO: what is this 80 for?\n#   count(title, page = linenumber %/% 80, sentiment) |&gt; \n#   # pivot wider data here\n#   pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n#   # get sentiment score\n#   mutate(sentiment = positive - negative) \n# # check head of data\n# head(aristotlesentiment)\n# # NOTE: looks good!\n\n\n# # ggplot of basic analysis of sentiment overtime\n# ggplot(aristotlesentiment, aes(page, sentiment, fill = title)) + \n#   # make bar graph, remove legend cause dont need\n#   geom_bar(stat = \"identity\", show.legend = FALSE) + \n#   # plot by book/title\n#   facet_wrap(~title, ncol = 3, scales = \"free_x\") \n# \n# \n# # plot the data\n# g = aristotlesentiment|&gt; \n#   # group by title\n#   group_by(title) |&gt; \n#   # get cumulative sentiment over time\n#   mutate(sentiment = cumsum(sentiment), page = page/max(page)) |&gt; \n#   # plot sentiment over time\n#   ggplot(aes(page, sentiment, colour = title)) + \n#   # make the line width bigger\n#   geom_line(linewidth = 1.25) + \n#   # labels\n#   labs(\n#     x = \"Percent of Total Pages (%)\",\n#     y = \"Cumulative Sentiment\",\n#     title = \"Trajectory of Sentiment Throughout Aristotle's books\",\n#     caption = \"Data Source: Project Gutenberg\"\n#   )\n# \n# # making transparent legend\n# transparent_legend = theme(legend.background = element_rect(fill = \"transparent\"), legend.key = \n#                              element_rect(fill = \"transparent\", color = \"transparent\"))\n# \n# # plot\n# g + \n#   # add transparent legend\n#   transparent_legend + \n#   # change colors\n#   scale_color_brewer(type = \"qual\") +\n#   # scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12), breaks = desired_order) +\n#   # make specific colors go to specific titles\n#   # scale_colour_manual(\n#   #   values = setNames(desired_colors, desired_order),\n#   #   breaks = desired_order\n#   # ) +\n#   # change x axis to percent\n#   scale_x_continuous(labels = scales::percent_format()) + \n#   # change theme to classic\n#   theme_classic() + \n#   # edit text\n#   theme(\n#     legend.position = c(0.2, 0.75), \n#     text = element_text(size = 12),\n#     plot.title = element_text(face = \"bold\", hjust = 0.5),\n#     legend.text = element_text(face = \"italic\")\n#     ) + \n#   # change legend postion\n#   guides(colour = guide_legend(title = \"Book\", override.aes = list(linewidth = 2)))\n\n\n\n\n### Clean fitzgerald Data, also do sentiment analysis but don't print ###\n\n\n\n# tokenize author\ntidy_fitzgerald = fitzgerald |&gt;\n  unnest_tokens(word, text)\n\n\n# filter author with stop words\ntidy_fitzgerald = tidy_fitzgerald |&gt; \n  # get rid of stop words\n  anti_join(stop_words, by = \"word\")\n\n\n# top words by book\ntop_fitzgerald_words = tidy_fitzgerald |&gt;\n  # count with word and group by title\n  count(word, title) |&gt;\n  # arrange\n  arrange(desc(n)) |&gt; \n  # group by title\n  group_by(title) \n# top_fitzgerald_words |&gt; slice(1:2)\n\n\n# # # top word sentiments with only words with sentiment\n# top_fitzgerald_words |&gt;\n#   # get rid of drop words\n#   filter(!word %in% dropwords) |&gt;\n#   inner_join(bing, by = join_by(word)) |&gt;\n#   slice(1:30)\n#   # NOTE: checked slices 1:30\n\n\n# Using this method to look at the text to determine if I should remove the word or not\n# or us a regex method\n# fitzgerald |&gt;\n#   filter(str_detect(text, \"gentlemen\"))\n\n\n# going to get rid of some words here\n# dropwords = c(\"majesty\", \"prisoner\", \"master\", \"excellency\")\n\n\n# # author sentiment\n# fitzgeraldsentiment = tidy_fitzgerald |&gt; \n#   # get rid of drop words\n#   # filter(!word %in% dropwords) |&gt; \n#   # join with bing to get sentiment\n#   inner_join(bing, by = join_by(word)) |&gt; \n#   # TODO: what is this 80 for?\n#   count(title, page = linenumber %/% 80, sentiment) |&gt; \n#   # pivot wider data here\n#   pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt; \n#   # get sentiment score\n#   mutate(sentiment = positive - negative) \n# # check head of data\n# head(fitzgeraldsentiment)\n# # NOTE: looks good!\n# \n# \n# # ggplot of basic analysis of sentiment overtime\n# ggplot(fitzgeraldsentiment, aes(page, sentiment, fill = title)) + \n#   # make bar graph, remove legend cause dont need\n#   geom_bar(stat = \"identity\", show.legend = FALSE) + \n#   # plot by book/title\n#   facet_wrap(~title, ncol = 3, scales = \"free_x\") \n# \n# \n# # plot the data\n# g = fitzgeraldsentiment|&gt; \n#   # group by title\n#   group_by(title) |&gt; \n#   # get cumulative sentiment over time\n#   mutate(sentiment = cumsum(sentiment), page = page/max(page)) |&gt; \n#   # plot sentiment over time\n#   ggplot(aes(page, sentiment, colour = title)) + \n#   # make the line width bigger\n#   geom_line(linewidth = 1.25) + \n#   # labels\n#   labs(\n#     x = \"Percent of Total Pages (%)\",\n#     y = \"Cumulative Sentiment\",\n#     title = \"Trajectory of Sentiment Throughout Fitzgerald's books\",\n#     caption = \"Data Source: Project Gutenberg\"\n#   )\n# \n# # making transparent legend\n# transparent_legend = theme(legend.background = element_rect(fill = \"transparent\"), legend.key = \n#                              element_rect(fill = \"transparent\", color = \"transparent\"))\n# \n# # plot\n# g + \n#   # add transparent legend\n#   transparent_legend + \n#   # change colors\n#   scale_color_brewer(type = \"qual\") +\n#   # scale_colour_manual(values = paletteer_d(\"ggprism::colors\", 12), breaks = desired_order) +\n#   # make specific colors go to specific titles\n#   # scale_colour_manual(\n#   #   values = setNames(desired_colors, desired_order),\n#   #   breaks = desired_order\n#   # ) +\n#   # change x axis to percent\n#   scale_x_continuous(labels = scales::percent_format()) + \n#   # change theme to classic\n#   theme_classic() + \n#   # edit text\n#   theme(\n#     legend.position = c(0.2, 0.25), \n#     text = element_text(size = 12),\n#     plot.title = element_text(face = \"bold\", hjust = 0.5),\n#     legend.text = element_text(face = \"italic\")\n#     ) + \n#   # change legend postion\n#   guides(colour = guide_legend(title = \"Book\", override.aes = list(linewidth = 2)))\n\n\nUltimately, the purpose of this analysis was to: 1. Differentiate between the three authors using relevant keywords when excluding names and proper nouns 2. Identify interesting patterns among authors and/or books.\nFor these reasons, certain words identified in earlier analyses were excluded from the current results presented. These excluded words included names within the books such as “anthony,” “gatsby,” or “dantès,” as well as common pronouns like “sir,” “madame,” and “dear.” Additionally, words such as “lord,” “queen,” and “monk” were removed because within the context of the data they typically functioned more as nouns referring to individuals.\nSee the “Code” section below for the full list of words excluded from the analysis.\n\n\nCode\n# Coming back and getting rid of words (mainly names)\nwords_i_dont_want = c(\"anthony\", \"gloria\", \"amory\", \"aramis\", \"porthos\",\"athos\",\"d’artagnan\",\"count\",\n                      \"de\", \"monte\",\"cristo\",\"villefort\",\"danglars\",\"madame\", \"morrel\", \"cornelius\", \"rosa\",\n                      \"monsieur\",\"dantès\", \"valentine\", \"franz\", \"sir\", \"friend\", \"albert\",\"girl\", \"king\",\n                      \"lord\",\"queen\",\"raoul\",\"mazarin\",\"father\", \"caderousse\", \"sire\", \"morcerf\",\"majesty\",\n                      \"milady\", \"friends\", \"cardinal\", \"loius\", \"monk\", \"colbert\", \"fouquet\", \"dear\",\"daisy\",\n                      \"tom\", \"gatsby\", \"grimaud\", \"planchet\", \"la\", \"tulip\", \"louis\", \"prince\", \"woman\",\n                      \"duke\", \"mordaunt\", \"paris\", \"gentlemen\", \"boxtel\", \"baerle\",\"rosalind\", \"gryphus\",\"maury\",\n                      \"charles\", \"le\", \"francs\", \"buckingham\",\"comte\", \"guiche\",\"edmond\", \"andrea\",\"noirtier\",\n                      \"malicorne\", \"poet\", \"ii\", \"baisemeaux\", \"montalais\", \"bonacieux\", \"chapter\",\"prisoner\")\n\n\nLDA was first performed with three topics, with results and thoughts below.\n\n\n\nCode\n### LDA Analysis of all Authors ###\n\n\n# Get bag of words\n# author 1 bow\ntidy_freq_dumas = tidy_dumas  |&gt; \n  dplyr::ungroup()  |&gt; \n  # count words\n  count(title, word, name = \"count\") |&gt; \n  # filter for numbers\n  filter(is.na(as.numeric(word))) |&gt; \n  # get rid of this novel\n  filter(title != \"The Wolf-Leader\") |&gt; \n  # retroactively get rid of these words\n  filter(!word %in% words_i_dont_want)\n\n\n# author 2 bow\ntidy_freq_aristotle = tidy_aristotle  |&gt; \n  dplyr::ungroup()  |&gt; \n  # count words\n  count(title, word, name = \"count\") |&gt; \n  # filter for numbers\n  filter(is.na(as.numeric(word))) |&gt; \n  # retroactively get rid of these words\n  filter(!word %in% words_i_dont_want)\n\n\n# author 1 bow\ntidy_freq_fitzgerald = tidy_fitzgerald  |&gt; \n  dplyr::ungroup()  |&gt; \n  # count words\n  count(title, word, name = \"count\") |&gt; \n  # filter for numbers\n  filter(is.na(as.numeric(word))) |&gt; \n  # retroactively get rid of these words\n  filter(!word %in% words_i_dont_want)\n\n\n# combine data\ndf_authors123 = rbind(tidy_freq_dumas, tidy_freq_aristotle, tidy_freq_fitzgerald) |&gt; \n  # get rid of stop words\n  anti_join(stop_words, by = \"word\") |&gt; \n  # arrange in descending order\n  arrange(desc(count))\n# head(df_authors123)\n\n\n# make Document Term Matrix\ndtm_author &lt;- df_authors123  |&gt; \n  cast_dtm(title, word, count)\n\n\n# Perform LDA on 3 topics\nlda_author &lt;- LDA(dtm_author, k = 3L, control = list(seed = 10))\n# lda_author\n\n\n# Look at words per topic\nbeta_author &lt;- tidy(lda_author, matrix = \"beta\")\n# beta_author\n\n\n# look at top terms\ntop_terms &lt;- beta_author  |&gt; \n  # group by topic\n  group_by(topic)  |&gt; \n  # show top 10\n  slice_max(beta, n = 15)  |&gt;  \n  ungroup()  |&gt; \n  # arrange by lowest beta\n  arrange(topic, -beta)\n# top_terms\n\n\n# plot top terms\ntop_terms |&gt; \n  # reorder terms based on beta and topic\n  mutate(term = reorder_within(term, beta, topic)) |&gt;\n  # change topic from numbers to legible lables\n  mutate(topic = case_when(topic == 1 ~ \"Topic 1\",\n                           topic == 2 ~ \"Topic 2\",\n                           topic == 3 ~ \"Topic 3\")) |&gt; \n  # begin plotting\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  # columns, or bars for expressing data\n  geom_col(show.legend = FALSE) +\n  # wrao based on the topic\n  facet_wrap(~ topic, scales = \"free_y\", nrow=2, ncol=2) +\n  scale_y_reordered() +\n  # labels for plot\n  labs(\n    x = \"Word Probability Per Topic (\\u03B2)\",\n    y = \"Word\",\n    title = \"Probability of Top Words Per Topic From a 3-Topic LDA\",\n    caption = \"Data Source: Project Gutenberg\"\n  ) + \n  # change theme\n  theme_linedraw() +\n  # edit text and legend\n  theme(\n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    legend.text = element_text(face = \"italic\")\n    ) +  \n  # change colors of plot\n    scale_fill_manual(\n    values = setNames(c(\"darkred\", \"darkgreen\",\"darkblue\"), c(\"Topic 1\", \"Topic 2\", \"Topic 3\")))\n\n\n\n\n\n\n\n\n\nExpressed above are the top 15 words within each topic determined by a 3-Topic LDA. Note there are no names, pronouns, or other words we omitted from the analysis. Topic 1 appears to be influenced by Aristotle’s books, as it contains words like “government,” “power,” and “voice.” Topic 2, and to a lesser degree Topic 3, contain many words in the past tense such as “replied” or “cried.” This interesting find suggests that some authors may have a preference for using the past tense more frequently than others.\nThere are some overlaps of words between topics such as “eyes” or “time.” In future work, particularly when classifying books, it might be of interest to examine why these overlaps are present and omit them from the analysis. However, for the purpose of this study we interpret these overlaps as potential commonalities between authors’ works and possible differences in word usage given specific contexts. For instance, in Dumas’ The Black Tulip, example quotes such as ““Of a tumult?” replied Cornelius, fixing his eyes on his perplexed” and “John, with tears in his eyes, wiped off a drop of the noble blood” showcase Dumas’ use of the word “eye” to describe eye actions. Conversely, in Aristotle’s Politics: A Treatise on Government, examples like “can see better with two eyes, and hear better with two ears” and “see that absolute monarchs now furnish themselves with many eyes” demonstrate Aristotle’s usage of “eye” as a noun rather than describing its actions. Countless more examples can be found in the text of words such as this.\nIt is worth noting that a word like “replied” likely differs from the “eye” example, as it is predominantly used when a person is responding to someone else. This common term is used frequently in books, so it was certainly interesting to see how it varies across topic and books as shown in the figure above and below.\n\n\nCode\n### LDA Analysis of all Authors Gamma Plot ###\n\n# check doc in each topic\ngamma_author &lt;- tidy(lda_author, matrix = \"gamma\")\n# gamma_author\n\n\n# get titles for each other\na1_titles = unique(tidy_dumas$title)\na2_titles = unique(tidy_aristotle$title)\na3_titles = unique(tidy_fitzgerald$title)\n# order for plot\nplot_order = c(a1_titles, a2_titles, a3_titles)\n\n\n# plot!\ngamma_author  |&gt; \n  # make title as factor of document, get plot order correct\n  mutate(title = factor(document, levels = plot_order))  |&gt; \n  # make new column author to use for facet wrap for more legible plot\n  mutate(author =  case_when(title %in% a1_titles ~ \"Alexandre Dumas\",\n                             title %in% a2_titles ~ \"Aristotle\",\n                             title %in% a3_titles ~ \"Scott F. Fitzgerald\")) |&gt; \n  # begin plot\n  ggplot(aes(x = title, y = gamma, fill = factor(topic))) +\n  # facet wrao by author with same y\n  facet_wrap(~author, scales = \"free_x\") +\n  # barplots\n  geom_col(width = 0.8) +\n  # labels\n  labs(\n    x = \"Book\",\n    y = paste(\"Topic Probability Per Book (\\u03B3)\"),\n    title = \"Proportion of Topics Per Book From a 3-Topic LDA\",\n    caption = \"Data Source: Project Gutenberg\",\n    fill = \"Topic\"\n  ) +\n  # change theme\n  theme_linedraw() +\n  # edit text and legend\n  theme(axis.text.x = element_text(angle = 55, hjust = 1),\n        plot.title = element_text(face = \"bold\", hjust = 0.5),\n        # legend.position = c(0.25, 0.3), \n        text = element_text(size = 12)) + \n  # change colors of fill\n  scale_x_discrete(labels = function(y) str_wrap(y, width = 20), expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"darkred\", \"darkgreen\",\"darkblue\")) +\n  # scale_fill_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # add some padding\n  coord_cartesian(xlim = c(0.5,  5 + 0.5), expand = FALSE)\n\n\n\n\n\n\n\n\n\nThe figure above expresses how each topic was represented in the various books and highlights some differences between the authors. After removing names and pronouns, we see that each topic does not correspond exclusively to each author. However, the figure does reveal some intriguing results. Fitzgerald’s books exclusively consist only of Topic 1, Dumas’ books heavily feature Topic 2, while Aristotle’s works are a mixture of both Topics 1 and 2. This suggests how Fitzgerald and Aristotle may have more similar writing styles, or at least use more words in common, compared to Dumas. It also suggests that Dumas and Fitzgerald have little similarities in their writing style and choice of words.\nInterestingly, Topic 3 resides almost only in Dumas’ The Count of Monte Cristo, Illustrated. The unique words of this topic, such as “return” and “heard,” differ entirely from the top 15 words of the other topics. This also suggests that words such as “door” and “house” are used more frequently in this specific book compared to others. Additionally, a bit of Topic 3 was located in three other books written by Dumas emphasizing some consistency within his writing.\nUpon further examination of the data, it was discovered that Aristotle never mentions the word “cried” in any of his five works analyzed. However, Topic 2, which has the term “cried” as its third highest coefficient, represents a majority of two of Aristotle’s books. This suggests that including more topics will likely enhance our understanding of the underlying connections of these authors and their works. For that reason, another LDA was conducted using five topics. Note that Topics 1-3 will not be identical to the previous analysis.\n\n\n\nCode\n### LDA with more than 3 topics ###\n\n\n# Perform LDA on 3 topics\nlda_author &lt;- LDA(dtm_author, k = 5L, control = list(seed = 10))\n# lda_author\n\n\n# Look at words per topic\nbeta_author &lt;- tidy(lda_author, matrix = \"beta\")\n# beta_author\n\n\n# look at top terms\ntop_terms &lt;- beta_author  |&gt; \n  # group by topic\n  group_by(topic)  |&gt; \n  # show top 10\n  slice_max(beta, n = 15)  |&gt;  \n  ungroup()  |&gt; \n  # arrange by lowest beta\n  arrange(topic, -beta)\n# top_terms\n\n\n\n# plot top terms\ntop_terms |&gt; \n  # reorder terms based on beta and topic\n  mutate(term = reorder_within(term, beta, topic)) |&gt;\n  # change topic from numbers to legible lables\n  mutate(topic = case_when(topic == 1 ~ \"Topic 1\",\n                           topic == 2 ~ \"Topic 2\",\n                           topic == 3 ~ \"Topic 3\",\n                           topic == 4 ~ \"Topic 4\",\n                           topic == 5 ~ \"Topic 5\")) |&gt; \n  # begin plotting\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  # columns, or bars for expressing data\n  geom_col(show.legend = FALSE) +\n  # wrao based on the topic\n  facet_wrap(~ topic, scales = \"free_y\", nrow=2, ncol=3) +\n  scale_y_reordered() +\n  # labels for plot\n  labs(\n    x = \"Word Probability Per Topic (\\u03B2)\",\n    y = \"Word\",\n    title = \"Probability of Top Words Per Topic From a 5-Topic LDA\",\n    caption = \"Data Source: Project Gutenberg\"\n  ) + \n  # change theme\n  theme_linedraw() +\n  # edit text and legend\n  theme(\n    text = element_text(size = 12),\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    legend.text = element_text(face = \"italic\"),\n    axis.text.x = element_text(size = 8)\n    ) +  \n  # change colors of plot\n    scale_fill_manual(\n    values = setNames(c(\"darkred\", \"darkgreen\",\"darkblue\", \"darkorange\", \"#4B0076\"), c(\"Topic 1\", \"Topic 2\", \"Topic 3\", \"Topic 4\", \"Topic 5\")))\n\n\n\n\n\n\n\n\n\nA total of five topics were chosen in attempt to understand the differences between authors and their respective books. The previous analysis, using three topics, had shown to encounter an issue where certain topic words did not align with the books they were associated with. Although topic values ranging from four to eight were explored, it was determined that five topics was the most interesting and worth investigating further.\nThe above figure showcases the top 15 words for each of the newly generated topics produced from the LDA analysis. Topic 1 displays a strong correlation with Aristotle’s works on politics and philosophy, evident through words such as “government,” “public,” and “law.” Topics 2 and 3 contain many similar words like “time” and “cried” which were predominately related to Dumas’ books in the previous analysis. Notably, Topic 2 introduces the word “whilst,” while Topic 3 introduces the word “honor” which serve as novel distinguishing terms for differentiating between authors. In Topic 4, the word “eyes” has the highest score along with words like “night” and “day” that seemed to be connected to Fitzgerald in the last analysis. This topic also introduced the words “suddenly” and “love.” Lastly, Topic 5 was nearly identical to Topic 3 in the previous analysis, which primarily consisted of Dumas’ The Count of Monte Cristo, Illustrated.\nTo reemphasize, although there are many word overlaps across topics they were retained in hope to better understand the interaction of words within each topic. One interesting word that may pique the readers’ curiosity is “ah,” which has been retained in the analysis. This word was exclusively found in the works of Alexandre Dumas, as shown in the figures above. It is written in the text as instances like ““Ah! ah!” within twelve hours, you say?” and ““Ah, ah!” said William to his dog, “it’s easy to see that she is a”” which are from The Black Tulip. The decision was made not to omit this word, as it was used more frequently during this time period and aids in distinguishing between authors.\n\n\nCode\n### LDA Analysis of all Authors Gamma Plot ###\n\n# check doc in each topic\ngamma_author &lt;- tidy(lda_author, matrix = \"gamma\")\n# gamma_author\n\n\n# plot!\ngamma_author  |&gt; \n  # make title as factor of document, get plot order correct\n  mutate(title = factor(document, levels = plot_order))  |&gt; \n  # make new column author to use for facet wrap for more legible plot\n  mutate(author =  case_when(title %in% a1_titles ~ \"Alexandre Dumas\",\n                             title %in% a2_titles ~ \"Aristotle\",\n                             title %in% a3_titles ~ \"Scott F. Fitzgerald\")) |&gt; \n  # begin plot\n  ggplot(aes(x = title, y = gamma, fill = factor(topic))) +\n  # facet wrao by author with same y\n  facet_wrap(~author, scales = \"free_x\") +\n  # barplots\n  geom_col(width = 0.8) +\n  # labels\n  labs(\n    x = \"Book\",\n    y = paste(\"Topic Probability Per Book (\\u03B3)\"),\n    title = \"Proportion of Topics Per Book From a 5-Topic LDA\",\n    caption = \"Data Source: Project Gutenberg\",\n    fill = \"Topic\"\n  ) +\n  # change theme\n  theme_linedraw() +\n  # edit text and legend\n  theme(axis.text.x = element_text(angle = 55, hjust = 1),\n        plot.title = element_text(face = \"bold\", hjust = 0.5),\n        # legend.position = c(0.25, 0.3), \n        text = element_text(size = 12)) + \n  # change colors of fill\n  scale_x_discrete(labels = function(y) str_wrap(y, width = 20), expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"darkred\", \"darkgreen\",\"darkblue\", \"darkorange\", \"#4B0076\")) +\n  # scale_fill_manual(values = paletteer_d(\"ggprism::colors\", 12)) +\n  # add some padding\n  coord_cartesian(xlim = c(0.5,  5 + 0.5), expand = FALSE)\n\n\n\n\n\n\n\n\n\nThe final figure above illustrates the representation of each topic across the various books while highlighting certain differences among the authors. When the number of topics was increased from three to five there was a complete separation among all authors, with Dumas exhibiting subcategories within his books.\nUnsurprisingly, Aristotle was exclusively represented by Topic 1, which was not found in any other novel. This phenomena likely occured because of words within the top 15 of Topic 1 such as “government,” “democracy,” and “oligarchy,” which closely resemble the themes explored in Aristotle’s works on politics. It was fascinating that these words primarily relate to Aristotle’s Politics: A Treatise on Government and The Athenian Constitution rather than his poetic works of The Poetics of Aristotle and Aristotle on the art of poetry. Further exploration may involve identifying distinct keywords that better differentiate these bodies of work from one another.\nFitzgerald was represented solely by Topic 4 in this analysis, which was characterized by words such as “night,” “day,” “suddenly,” and “love.” Considering the selected works of this author, it comes as no surprise that these words achieve high scores for Fitzgerald. However, if words such as “woman,” “girl,” or “gentlemen,” which were excluded from the analysis, were included, the books would likely better differentiate into different topics.\nMost interestingly, despite being compared to Aristotle and Fitzgerald’s works, Dumas’ five works are divided into three topics. The Count of Monte Cristo, Illustrated forms its own distinctive topic, Topic 5, corresponding exactly to Topic 3 in the previous analysis. Topic 3 was mainly found in The Three Musketeers and Ten Years Later, while Topic 4 was predominately in The Black Tulip and Twenty Years After. It was expected that the D’Artagnan Romances trilogy would fall under a single topic, and while this was mostly the case, Twenty Years After contains a significant portion of Topic 2. Topic 2 was fully associated with The Black Tulip, an adventure novel by Dumas characterized with distinguishing words such as “whilst” and “van.” Overall, it was satisfying to observe that this analysis successfully achieves its objective of distinguishing between authors and their works, while also revealing aforementioned subcategories within Dumas’ books."
  },
  {
    "objectID": "blog/2023-09-20-sentiment-analysis-example/index.html#conclusion",
    "href": "blog/2023-09-20-sentiment-analysis-example/index.html#conclusion",
    "title": "Sentiment and Topic Model Analysis of Alexandre Dumas & Others",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the authors’ works exhibit notable differences as revealed through this analysis. Specific words unique to each author were successfully identified, indicating distinct writing styles among Dumas, Aristotle, and Fitzgerald. Moreover, the analysis successfully distinguished all three authors and discovered the previously mentioned subcategories within Dumas’ works using a five-topic LDA model. Future research could focus on identifying distinguishing words between books authored by Aristotle and Fitzgerald."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html",
    "href": "blog/2023-10-20-pca-on-images/index.html",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "",
    "text": "In this project, we will embark on an adventure using Principal Component Analysis on images of various flags of countries/regions. Hopefully by the end you will have a better understanding of PCA and how we can utilize it to look at images in R!\nNote: I condensed the R code to “Code” sections throughout for scrolling purposes. Feel free to click on these sections to look at each step, or change to “Show All Code” in the top right of the webpage!"
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#introduction",
    "href": "blog/2023-10-20-pca-on-images/index.html#introduction",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "Introduction",
    "text": "Introduction\nPrincipal Components Analysis (PCA) is a well-established technique to reduce the dimensions or features of a dataset while preserving the maximum amount of variation. This technique is widely used for pattern recognition, signal processing, and machine learning. PCA is also a useful tool in denoising, visualization, and classification of large datasets given features are linearly related. For example, imagine you have a genomics dataset with 10000+ genes and 5000+ observations. That’s a lot of data! However, not all of those genes will be important as likely many of them are correlated, or express similar variation, as others. PCA is a great tool to reduce the 10000+ dimensions to a smaller number, eliminating redundancy by reducing the dimensions of the dataset for more downstream analysis.\n\n\n\n\n\nImage Source\n\n\n\nPCA is heavily based on linear algebra concepts. Essentially, PCA computes the eigenvectors of the covariance matrix of the data and sorts them by their eigenvalue (which correspond to the explained variance for that individual principal component). The principal components (PC) are then computed as linear combinations of the original variables using the eigenvectors. It sounds complicated, but I promise you it’s not as hard to understand as you think. This article was a great resource for me when I was first trying to figure it out. Unfortunately, I won’t go into much of the algebra behind PCA, so I encourage you to research it more! You can see this really cool animation above expressing what PCA is doing in two dimensions. Briefly, it is finding the direction that maximizes the variance of the blue dots which can also be viewed as minimizing the residuals of the blue dots to a line."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#data",
    "href": "blog/2023-10-20-pca-on-images/index.html#data",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "Data",
    "text": "Data\nIn this project specifically, instead of working with gene expressions or large datasets with numerous features, we are looking at images of flags from different countries. The goal is to apply PCA to these flags to assess its effectiveness in preserving variation and possibly discover interesting patterns or insights in the principal components themselves.\nTo begin our exploration, we need to install and load some R packages. These packages will equip us with the necessary tools for working with data frames, functions, file paths, reading images, and image processing.\n\n\n\nCode\n### install and read in packages\n\nlibrary(tidyverse) # for dataframes\nlibrary(here) # for use of others if downloaded\nlibrary(png) # for reading png images\nlibrary(magick) # for reading in images\nlibrary(imager) # for plotting\n\n\nNow that we have the necessary packages, let’s grab the flag data from here. This dataset contains flags of varying sizes within different folders, however we are going to look specifically at the “/png250px/” folder. This folder has 255 flags from various countries and regions.\n\n\n\nCode\n### Grabbing the data\n\n# url to flag data, it is in zip file\nurl = \"https://github.com/hampusborgos/country-flags/archive/refs/heads/main.zip\"\n\n# specify the file name and location where you want to save the file on your computer\nfile_name = \"flags.zip\"\nfile_path = here()\n\n# use the download.file() function\ndownload.file(url, paste(file_path, file_name, sep = \"/\"), mode = \"wb\")\n\n# unzip zip file\nunzip(paste0(here(), \"/flags.zip\"), exdir = here())\n\n# get file names\nfiles = list.files(here(\"country-flags-main/png250px/\"), full.names = TRUE)\n\n# Read each image file in the folder\nimage_list = lapply(files, image_read)\n# image_list = lapply(files, readPNG)\n\n\nFor PCA to work soundly, it is essential that the dataset’s dimensions remain consistent. Of course, that’s not the case in this dataset where we see all flags have the same height (250px) but very different widths. Hence, we need to resize each flag to be the same height and width. I use the magick R package to read in the images and the “image_scale” along with the “image_convert” functions to transform the images into size 250x250x3 (representing height, width, and color channel). These resized images are then saved in the folder “/resized_png250px/.” To enable PCA analysis, the images are converted from matrix to vector format. This involves flattening the 250x250x3 image matrix into a single vector of size 1x187500 (250 x 250 x 3). The individual vectors for each image are then combined into a single matrix of dimensions 255x187500. This variable, “image_matrix” is created and saved as flags_matrix.RDS. Also to note, this matrix is country/region flag image x pixel of flag image.\n\n\nCode\n# NOTE: so each image is the same height but very different widths lol. Let's change that\n\n# Set height and width I want\nmax_height = 250\nmax_width = 250\n\n# Resize all the images to the specified height and width\n# had to add matte=FALSE to get rid of extra channel\nresized_images = lapply(image_list, function(im) {\n  image_convert(image_scale(im, paste0(max_width, \"x\", max_height, \"!\")), format = \"png\", matte=FALSE)\n})\n\n\n# Create the directory to save the resized images\ndir.create(\"resized_png250px\")\n\n# Save the resized images with the same names as the original files\nfor (i in seq_along(resized_images)) {\n  # Extract the file name from the full path\n  file_name = basename(files[i])\n  # make the file path for saving the resized image\n  save_path = file.path(\"resized_png250px\", file_name)\n  # Write the resized image to the specified file path\n  image_write(resized_images[[i]], save_path)\n}\n\n\n# now grab them with png package\nresized_files = list.files(here(\"blog/2023-10-20-pca-on-images/resized_png250px/\"), full.names = TRUE)\n\n# use function readPNG\nimgs_final = lapply(resized_files, readPNG)\n\n# QC: check each image is same dimensions\n# for (i in seq_along(imgs_final)) {\n#   dimensions = dim(imgs_final[[i]])\n#   cat(\"Image\", i, \"Dimensions:\", dimensions[1], \"x\", dimensions[2], \"x\", dimensions[3], \"\\n\")\n# }\n\n# great!\n\n# Get the number of images in the list\nnum_images = length(image_list)\n\n# Create an empty matrix to store the flattened images\nimage_matrix = matrix(NA, nrow = num_images, ncol = 250 * 250 * 3)\n\n# Flatten each image and store it as a column in the matrix\nfor (i in 1:num_images) {\n  # get the flatten vector\n  flattened_image = as.vector(imgs_final[[i]])\n  # add to matrix\n  image_matrix[i,] = flattened_image\n}\n\n# save as .rds file\n# saveRDS(image_matrix, file = here(\"flags_matrix.RDS\"))\n\n\n# plot images\n\n# want to add all images to this dataframe for ggplot\nall_images = data.frame()\n\n# for loop getting 5 examples\nfor (i in seq(1,190,20)) {\n  # getting image - making cimg from imager package and then df\n  img = as.data.frame(as.cimg(imgs_final[[i]]), wide = \"c\") |&gt; \n    # # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # # make class labels\n    mutate(img_num = paste(\"Image\",i))\n  \n  all_images = rbind(all_images, img)\n  \n  # make the levels of img variable what i want\n  all_images$img_num = factor(all_images$img_num, levels = sprintf(\"Image %d\", seq(1, 190, 20)))\n\n}\n\n# plot image\nprint(ggplot(all_images,aes(y,x))+\n  # getting rgb\n  geom_raster(aes(fill=rgb_value))+\n  # fill image\n  scale_fill_identity() +\n  # facet_wrap\n  facet_wrap(.~ img_num, nrow = 2, ncol = 5) +\n  # reverse y axis\n  scale_y_reverse() + \n  # get rid of theme\n  theme_void() +\n  # bigger font size\n  theme(strip.text = element_text(size = 12)))\n\n\n\n\n\n\n\n\n\nHere are ten examples of the flag image data we are using."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#pca-on-the-training-data",
    "href": "blog/2023-10-20-pca-on-images/index.html#pca-on-the-training-data",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "PCA on the training data",
    "text": "PCA on the training data\nTechnically our “image_matrix” variable could be input straight into PCA the way it is. However, we are going to split the 255 images into a training and test set. Why do we do this? Well, for one it was part of the project description 😅 But really we do this to see how well PCA will predict, or project, the test data using only information from the training data. Our training set we take to be 75% of the data (191 flags) leaving 25% of data being the test images (64 flags). Finally, we are ready to conduct PCA on the training data! Note that we are setting center=TRUE and scale.=False. Briefly, the center hyperparameter shifts the data to be zero centered (subtracting the mean from each column) while scale will make the data have unit variance (correlation instead of covariance PCA). Centering is crucial for PCA to perform correctly, however scaling is dataset dependent. Take a look at the prcomp function description for more information.\n\n\n\nCode\n### Do PCA analysis ###\n\n# Set the seed \nset.seed(123)\n\n# Get the number of images in the list - for some reason needed to add this to this code chunk\nnum_images = dim(image_matrix)[1]\n\n# Calculate the number of columns for training data\ntrain_columns = floor(0.75 * num_images)\n\n# Randomly select indices for the training data\ntrain_indices = sample(1:num_images, train_columns)\n\n# Get the test indices\ntest_indices = setdiff(1:num_images, train_indices)\n\n# Split the image matrix into training and test data\ntrain_data = image_matrix[train_indices, ]\ntest_data = image_matrix[-train_indices, ]\n\n# Perform PCA on the training data, centering data but NOT scaling\npca_result = prcomp(train_data, center=TRUE, scale. = FALSE)\n\n\nLet’s extract some key information form the prcomp function. We are able to calculate the proportion of variance explained from each PC by looking at the \\(sdev\\) value, aka the standard deviations of the principal components, by squaring them and dividing by the sum of the squared \\(sdev\\) values. To note, \\(sdev^2\\) is actually equal to the eigenvalues of the dataset! We can then get the cumulative variation as the number of PC components increase, leading to the plot you see below. Here we see that as PCs increase so does the cumulative variance preserved/explained. This is an excellent visualization to check how much each PC contributes to preserving overall variation. In our case we see that with just 67 PCs we can explain 95% of the variation in the dataset!\n\n\nCode\n# Extract the proportion of variance explained by each principal component\nvariance_explained = pca_result$sdev^2 / sum(pca_result$sdev^2)\n\n# Calculate the cumulative percentage of variance explained\ncumulative_variance = cumsum(variance_explained) * 100\n\n# get pcs getting more than 95% of the data\npcs_for_95 = which(cumsum(variance_explained) &gt;= 0.95)[1]\n\n# Create a tibble for plotting\ndata_plot = tibble(\n  # x axis\n  num_components = 1:length(cumulative_variance),\n  # cumvar\n  cumulative_variance = cumulative_variance\n)\n\n# plot using ggplot2\nggplot(data_plot, aes(x = num_components, y = cumulative_variance)) +\n  # make line plot\n  geom_line() +\n  # add points\n  geom_point() +\n  # adding line for which PC number we are getting\n  geom_vline(xintercept = pcs_for_95, color = \"red\", linetype = \"dashed\") +\n  # adding text for PC im getting\n  annotate(\"text\", x = 70, y = 85, label = paste(\"95% variation explained\\nwith\", as.character(pcs_for_95), \"PCs\"), hjust = 0, vjust = 0, color = \"black\", size = 4) +\n  # x axis every 20\n  scale_x_continuous(breaks = seq(0, max(data_plot$num_components), by = 20)) +\n  # y axis every 20\n  scale_y_continuous(breaks = seq(0, 100, by = 20)) +\n  # labels\n  labs(x = \"Number of Principal Components\",\n       y = \"Cumulative Variance Explained (%)\",\n       title = \"Cumulative Variance Explained with Additional Principal Components\") +\n  # theme\n  theme_bw() +\n  # change text vars\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        axis.text = element_text(size = 10))\n\n\n\n\n\n\n\n\n\nAnother popular plot used to visualize PCs and their explained variation is a scree plot. Check out the wiki page for more information.\nSo we were able to successfully perform PCA on our image data… what now? Well, let’s see how well we can reconstruct the data using only the first 67 PCs, which contained 95% of the variance. To achieve this, we need to do a bit of matrix multiplication as well as add the mean back to un-center the data.\nThe PCA reconstructed data = the PC scores (matrix dimensions 191x191) x the transpose of the eigenvectors (matrix dimensions 191x187500) + mean\nIf we wanted to take only the top k PCs, which in our case I wanted the top 67, then we simply subset the PC scores and eigenvectors from 191 to 67. I then reconstruct the image matrix from the single image vector, normalize the values from 0-1, and save all the newly reconstructed image matrices in a single list.\n\n\nCode\n### reconstructing training data ###\n\n# use pca_results to reconstruct training data fully\nreconstructed_train_data_allpcs = pca_result$x %*% t(pca_result$rotation)\n# scale data back to center\nreconstructed_train_data_allpcs = scale(reconstructed_train_data_allpcs, center = -pca_result$center, scale = FALSE)\n\n# now use just 95% variation explained pcs\nreconstructed_train_data_95 = pca_result$x[,1:pcs_for_95] %*% t(pca_result$rotation[,1:pcs_for_95])\n# scale data back to center\nreconstructed_train_data_95 = scale(reconstructed_train_data_95, center = -pca_result$center, scale = FALSE)\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", length(length(train_indices)))\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:length(train_indices)) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(reconstructed_train_data_allpcs[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Function to normalize the matrix \nnormalize_matrix = function(matrix) {\n  min_value = min(matrix)\n  max_value = max(matrix)\n  normalized_matrix = (matrix - min_value) / (max_value - min_value)\n  return(normalized_matrix)\n}\n\n# Normalize each matrix in the list\nflags_reconstructed_train_data_allpcs = lapply(matrices_250x250x3, normalize_matrix)\n\n# now do for 95% variation\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", length(length(train_indices)))\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:length(train_indices)) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(reconstructed_train_data_95[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Normalize each matrix in the list\nflags_reconstructed_train_data_95 = lapply(matrices_250x250x3, normalize_matrix)\n\n\nNow let’s view how well PCA reconstructed the data! I utilized ggplot to plot three versions of five different flags: the original resized training image, the reconstructed resized training image with all PCs, the reconstructed resized training image with 67 PCs.\n\n\nCode\n# for loop getting 5 examples\nfor (i in 1:5) {\n  \n  # getting image - making cimg from imager package and then df\n  img1 = as.data.frame(as.cimg(imgs_final[[train_indices[i]]]), wide = \"c\")  |&gt; \n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"Original Image\")\n  \n  # getting image - making cimg from imager package and then df\n  img2 = as.data.frame(as.cimg(flags_reconstructed_train_data_allpcs[[i]]), wide = \"c\") |&gt;\n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"Fully Reconstructed Image\")\n\n  # getting image 1 - making cimg from imager package and then df\n  img3 = as.data.frame(as.cimg(flags_reconstructed_train_data_95[[i]]), wide = \"c\") |&gt;\n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"67 PCs Reconstructed Image\")\n  \n  # combining to all images\n  all_images = rbind(img1, img2, img3)\n  \n  # make the levels of img variable what i want\n  all_images$image_type = factor(all_images$image_type, levels = c(\"Original Image\", \"Fully Reconstructed Image\",\"67 PCs Reconstructed Image\"))\n\n  # plot image\n  print(ggplot(all_images,aes(y,x))+\n    # getting rgb\n    geom_raster(aes(fill=rgb_value))+\n    # fill image\n    scale_fill_identity() +\n    # reverse y axis\n    scale_y_reverse() + \n    # wrap by image\n    facet_wrap(.~image_type) +\n    # get rid of theme\n    theme_void() +\n    # bigger font size\n    theme(strip.text = element_text(size = 12)))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you think? Is 67 PCs enough to reconstruct the data? Since all 191 PCs were used to reconstruct the flags in the middle, we expect it to look identical to the original image as it preserves 100% of the variation. However, using only 67 PCs, or 95% of the variation of the flag images, we see more grainy images with artifacts from other flags. A lot of the reconstructed flags seems to have X’s on them along with a symbol from another flag within the dataset. If my goal was to compress these images by reducing the dimensions using PCA, I personally would use more PCs to do so. However, these reconstructed flags might be enough if one were trying to do classification or another machine learning task. So it really depends on what the goal of your project is to determine how many PCs to keep."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#pca-on-the-test-data",
    "href": "blog/2023-10-20-pca-on-images/index.html#pca-on-the-test-data",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "PCA on the test data",
    "text": "PCA on the test data\nNow let’s see how well the PCA on the training data performs on the testing data! To reconstruct the test data, we first use the “predict” function in R to obtain the PC scores for the training data using the PCA results from the training data. We can then use the same formula we used for training set to get the reconstructed data for the testing set. Finally, we can plot these flags using ggplot to visualize the reconstructed data.\n\n\nCode\n### Project testing data ###\n\n# first predict the testing data\ntest_data_projected = predict(pca_result, newdata = test_data)\n\n# Reconstruct the test_data from the projected data using all pcs\nreconstructed_test_data_allpcs = test_data_projected %*% t(pca_result$rotation)\n\n# reconstruct using 95%\nreconstructed_test_data_95 = test_data_projected[,1:pcs_for_95] %*% t(pca_result$rotation[,1:pcs_for_95])\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", length(length(test_indices)))\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:length(test_indices)) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(reconstructed_test_data_allpcs[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Normalize each matrix in the list\nflags_reconstructed_test_data_allpcs = lapply(matrices_250x250x3, normalize_matrix)\n\n\n# now with 95%\n\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", length(length(test_indices)))\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:length(test_indices)) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(reconstructed_test_data_95[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Normalize each matrix in the list\nflags_reconstructed_test_data_95 = lapply(matrices_250x250x3, normalize_matrix)\n\n# for loop getting 5 examples\nfor (i in 1:5) {\n  \n  # getting image - making cimg from imager package and then df\n  img1 = as.data.frame(as.cimg(imgs_final[[test_indices[i]]]), wide = \"c\")  |&gt; \n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"Original Image\")\n  \n  # getting image - making cimg from imager package and then df\n  img2 = as.data.frame(as.cimg(flags_reconstructed_test_data_allpcs[[i]]), wide = \"c\") |&gt;\n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"Fully Reconstructed Image\")\n\n  # getting image 1 - making cimg from imager package and then df\n  img3 = as.data.frame(as.cimg(flags_reconstructed_test_data_95[[i]]), wide = \"c\") |&gt;\n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt;\n    # make class for facet_wrap\n    mutate(image_type = \"67 PCs Reconstructed Image\")\n  \n  # combining to all images\n  all_images = rbind(img1, img2, img3)\n  \n  # make the levels of img variable what i want\n  all_images$image_type = factor(all_images$image_type, levels = c(\"Original Image\", \"Fully Reconstructed Image\",\"67 PCs Reconstructed Image\"))\n\n  # plot image\n  print(ggplot(all_images,aes(y,x))+\n    # getting rgb\n    geom_raster(aes(fill=rgb_value))+\n    # fill image\n    scale_fill_identity() +\n    # reverse y axis\n    scale_y_reverse() + \n    # wrap by image\n    facet_wrap(.~image_type) +\n    # get rid of theme\n    theme_void() +\n    # bigger font size\n    theme(strip.text = element_text(size = 12)))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow you can observe that even when using all 191 PCs to reconstruct the test data, the result isn’t perfect. However, this makes sense! Since PCA was not performed on the testing data it likely contains new and unseen information for the algorithm. In other words, there exists variation within this new test dataset that was not present in the training dataset. Therefore, achieving a perfectly reconstructed image is not possible.\nI would argue that the difference between using 191 PCs or 67 PCs in the test case isn’t very significant. Particularly when comparing it to the notable differences in the training dataset, the reconstruction quality doesn’t appear to be too bad. There is one interesting observation we can make regarding the last image featuring the trident in the center. It’s clear that the reconstructed data was unable to accurately recreate this symbol. Instead, we see a depiction of a star in the middle image and a distinct-ish symbol in the far-right image. This suggests that there likely wasn’t a similar symbol present in the training dataset, causing PCA to struggle in reconstructing it."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#visualizing-the-principal-components",
    "href": "blog/2023-10-20-pca-on-images/index.html#visualizing-the-principal-components",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "Visualizing the principal components",
    "text": "Visualizing the principal components\nTo finish up, lets take a look at the top 10 PCs in image form. To do this, we simply look at the transpose of the rotation variable within pca_results. We transform this single vector into image form via the same process as before, and visualize using ggplot.\n\n\n\nCode\n### Plot PCs ###\n\n# get pcs\nprincipal_components = t(pca_result$rotation)\n\n# Create an empty list to store the 250x250x3 matrices\nmatrices_250x250x3 = vector(\"list\", dim(principal_components)[1])\n\n# Generate a 250x250x3 matrix for each row in image_matrix\nfor (i in 1:dim(principal_components)[1]) {\n  # flattened_image = as.vector(imgs_final[[i]])\n  matrix_250x250x3 = array(principal_components[i,], dim = c(250, 250, 3))\n  matrices_250x250x3[[i]] = matrix_250x250x3\n}\n\n# Normalize each matrix in the list\nflags_principal_components = lapply(matrices_250x250x3, normalize_matrix)\n\n# want to add all images to this dataframe for ggplot\nall_images = data.frame()\n\n# for loop getting 5 examples\nfor (i in 1:10) {\n  # getting image - making cimg from imager package and then df\n  img = as.data.frame(as.cimg(flags_principal_components[[i]]), wide = \"c\")  |&gt; \n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt; \n    # make class labels\n    mutate(img_num = paste(\"PC\",i))\n  \n  all_images = rbind(all_images, img)\n  \n  # make the levels of img variable what i want\n  all_images$img_num = factor(all_images$img_num, levels = c(\"PC 1\",\"PC 2\",\"PC 3\",\"PC 4\",\"PC 5\",\n                                                                   \"PC 6\",\"PC 7\",\"PC 8\",\"PC 9\",\"PC 10\"))\n}\n\n# plot image\nprint(ggplot(all_images,aes(y,x))+\n  # getting rgb\n  geom_raster(aes(fill=rgb_value))+\n  # fill image\n  scale_fill_identity() +\n  # facet_wrap\n  facet_wrap(.~ img_num, nrow = 2, ncol = 5) +\n  # reverse y axis\n  scale_y_reverse() + \n  # get rid of theme\n  theme_void() +\n  # bigger font size\n  theme(strip.text = element_text(size = 12)))\n\n\n\n\n\n\n\n\n\nPersonally, I think this is the coolest part! It’s amazing to visually observe the variations captured by each PC. One of the most compelling examples that caught my attention is the comparison between PC6 and PC9. Both PCs capture the flag symbol in the top left corner and exhibit similar colors. However, PC6 splits its colors horizontally, while PC9 splits them vertically. They complement each other in a nice way, showcasing the diverse ways information is reflected across the PCs! Some other PCs, such as PC3 and PC4, have a striking resemblance to specific flag images they were trained on. On the other hand, PCs like PC7 and PC10 seem to combine elements from two or more flags in the dataset, creating cool hybrid representations.\nThese results actually made me curious about what the appearance of the PCs beyond the top 10 would look like. So let’s plot ten of them side by side and compare! This will hopefully provide further insights into the patterns and variations captured by these additional PCs.\n\n\nCode\n### Plot PCs ###\n\n# want to add all images to this dataframe for ggplot\nall_images = data.frame()\n\n# for loop getting 5 examples\nfor (i in seq(11,191,20)) {\n  # getting image - making cimg from imager package and then df\n  img = as.data.frame(as.cimg(flags_principal_components[[i]]), wide = \"c\")  |&gt; \n    # get rgb channels\n    mutate(rgb_value = rgb(c.1, c.2, c.3)) |&gt; \n    # make class labels\n    mutate(img_num = paste(\"PC\",i))\n  \n  all_images = rbind(all_images, img)\n  \n  # make the levels of img variable what i want\n  all_images$img_num = factor(all_images$img_num, levels = sprintf(\"PC %d\", seq(11, 191, 20)))\n}\n\n# plot image\nprint(ggplot(all_images,aes(y,x))+\n  # getting rgb\n  geom_raster(aes(fill=rgb_value))+\n  # fill image\n  scale_fill_identity() +\n  # facet_wrap\n  facet_wrap(.~ img_num, nrow = 2, ncol = 5) +\n  # reverse y axis\n  scale_y_reverse() + \n  # get rid of theme\n  theme_void() +\n  # bigger font size\n  theme(strip.text = element_text(size = 12)))\n\n\n\n\n\n\n\n\n\nExcept for PC11, which is only one rank away from the top 10, these PCs exhibit distinct characteristics when compared to the top PCs. Rather than focusing on the overall color of the flag images, they appear to emphasize patterns within the dataset. Interestingly, the final PC essentially corresponds to a completely black image. Checking our variance_explained variable, we find it accounts for 7.620894e-35, which is approximately 0% of the dataset highlighting its lack of information. Additionally, many of the later PCs feature specific symbols like stars and shields, which may explain why our reconstructed test data failed to capture the trident but instead exhibited various other symbols such as a star. This serves as an important reminder that predicting new data can be challenging, particularly when it introduces novel variations or, in our case, flag symbols in images that were previously unseen by PCA. This concept holds true across most (if not all) machine learning applications, especially deep learning where the availability of comprehensive training datasets can be a major constraint."
  },
  {
    "objectID": "blog/2023-10-20-pca-on-images/index.html#final-thoughts",
    "href": "blog/2023-10-20-pca-on-images/index.html#final-thoughts",
    "title": "Performing Principal Component Analysis on Flag Images in R",
    "section": "Final thoughts",
    "text": "Final thoughts\nHonestly, this was a really fun project to do! It really helped me understand PCA better and how it can be utilized with images. I also was able to work on my R skills using matrix multiplication, imaging packages, and ggplot! I hope this project will help if you are struggling to understand how to do PCA on images in R. I did some searching and it was hard to find a good resource that went through step-by-step of the process… so hopefully this helped and my code is commented well enough for anyone to understand 😊 Let me know if you have any questions about the process or why I did what I did!\nThanks for reading 🥳🥳\nCaleb 🎲"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks/Posters",
    "section": "",
    "text": "2023\nPosters:\n\n“Phenotyping of Heterogenous Live Cell Motility and Morphology,” Dr. M. Judah Folkman Research Day, Boston Children’s Hospital & Harvard Medical School, Boston, MA, 2023.\n\n\n\n\n\n\n2022\nTalks:\n\n“Machine Learning Approaches Applied to the Prediction of Covid-19 Spread and Cell Motility Phenotyping,” Vascular Biology Program Work in Progress, Boston Children’s Hospital & Harvard Medical School, Boston, MA, 2022.\n\n\n\n\n\n\n“Deconvolution of Cellular Heterogeneity for Sub-Type Discovery by Analyzing Feature Variation,” Vascular Biology Program Work in Progress, Boston Children’s Hospital & Harvard Medical School, Boston, MA, 2022.\n\n\n\n\n\nPosters:\n\n“Deep-Hetero: A Deep Metric Learning with UMAP-based Clustering Approach for Identifying Heterogeneity in Cells,” Dr. M. Judah Folkman Research Day, Boston Children’s Hospital & Harvard Medical School, Boston, MA, 2022.\n\n\n\n2021\nPosters:\n\n“Ultrasound Microbubble Tumor Analysis,” Focused Ultrasound Foundation Summer Intern Presentations, Focused Ultrasound Foundation, Charlottesville, VA, 2021."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Caleb Hallinan",
    "section": "",
    "text": "Second-Year PhD Candidate in Biomedical Engineering at Johns Hopkins University in the JEFworks Lab\n\n\n\nPrevious & Current Research Interests:\nComputational analysis of spatial transcriptomics data\nMorphodynamic analysis of live cell imaging\nDeveloping user-friendly computational software\n\n\nEducation:\nJohns Hopkins University, Baltimore, MD\nPhD in Biomedical Engineering | Aug 2023 - Present\nUniversity of Virginia | Charlottesville, VA\nB.A. in Statistics and Biology | Aug 2017 - May 2021\n\n\nExperience:\nBoston Children’s Hospital & Harvard Medical School | Boston, MA\nResearch Assistant II | Sept 2021 - June 2023\n\n\n\nWebsite last updated on: April 4, 2025"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "An Analysis on my 2023 Fantasy Football League (So Far)\n\n\n\nproject\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nCaleb Hallinan\n\n\n\n\n\n\n\n\n\n\n\n\nPerforming Principal Component Analysis on Flag Images in R\n\n\n\nproject\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nCaleb Hallinan\n\n\n\n\n\n\n\n\n\n\n\n\nSentiment and Topic Model Analysis of Alexandre Dumas & Others\n\n\n\nproject\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nCaleb Hallinan\n\n\n\n\n\n\n\n\n\n\n\n\nMight try blogging…\n\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nCaleb Hallinan\n\n\n\n\n\nNo matching items"
  }
]